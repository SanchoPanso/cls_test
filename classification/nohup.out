/home/timssh/miniconda3/envs/tagging/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:197: UserWarning: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.
  rank_zero_warn(
wandb: Currently logged in as: timasaviin (tsu). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.15.5
wandb: Run data is saved locally in /home/timssh/ML/TAGGING/DATA/models/wandb/run-20230716_155341-version_1_all_eff_36_0.001
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run version_1_all_eff_36_0.001
wandb: â­ï¸ View project at https://wandb.ai/tsu/tits_size
wandb: ğŸš€ View run at https://wandb.ai/tsu/tits_size/runs/version_1_all_eff_36_0.001
/home/timssh/miniconda3/envs/tagging/lib/python3.10/site-packages/lightning_fabric/connector.py:562: UserWarning: 16 is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!
  rank_zero_warn(
Using 16bit Automatic Mixed Precision (AMP)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]

  | Name          | Type               | Params
-----------------------------------------------------
0 | model         | EfficientNet       | 20.2 M
1 | transform     | DataAugmentation   | 0     
2 | sigmoid       | Sigmoid            | 0     
3 | cross_entropy | BCEWithLogitsLoss  | 0     
4 | accuracy1     | MultilabelAccuracy | 0     
5 | F1_M          | MultilabelF1Score  | 0     
6 | F1_m          | MultilabelF1Score  | 0     
7 | F1_N          | MultilabelF1Score  | 0     
-----------------------------------------------------
20.2 M    Trainable params
0         Non-trainable params
20.2 M    Total params
80.730    Total estimated model params size (MB)
/home/timssh/ML/TAGGING/CLS/classification/script.sh: line 13: 494755 Killed                  python train_wb.py --cat tits_size --batch 36 --arch eff --mode all --decay 0.001 > ~/nohup.out
/home/timssh/ML/TAGGING/CLS/classification/script.sh: line 14: 500076 Killed                  python train_wb.py --cat hair_type --batch 36 --arch eff --mode all --decay 0.001 > ~/nohup.out
/home/timssh/ML/TAGGING/CLS/classification/script.sh: line 15: 500080 Killed                  python train_wb.py --cat sex_positions --batch 36 --arch eff --mode all --decay 0.001 > ~/nohup.out
/home/timssh/ML/TAGGING/CLS/classification/script.sh: line 16: 500086 Killed                  python train_wb.py --cat hair_color --batch 36 --arch eff --mode all --decay 0.001 > ~/nohup.out
Global seed set to 1
/home/timssh/miniconda3/envs/tagging/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:197: UserWarning: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.
  rank_zero_warn(
wandb: Currently logged in as: timasaviin (tsu). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.15.5
wandb: Run data is saved locally in /home/timssh/ML/TAGGING/DATA/models/wandb/run-20230716_163151-version_2_all_eff_36_0.001
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run version_2_all_eff_36_0.001
wandb: â­ï¸ View project at https://wandb.ai/tsu/tits_size
wandb: ğŸš€ View run at https://wandb.ai/tsu/tits_size/runs/version_2_all_eff_36_0.001
/home/timssh/miniconda3/envs/tagging/lib/python3.10/site-packages/lightning_fabric/connector.py:562: UserWarning: 16 is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!
  rank_zero_warn(
Using 16bit Automatic Mixed Precision (AMP)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]

  | Name          | Type               | Params
-----------------------------------------------------
0 | model         | EfficientNet       | 20.2 M
1 | transform     | DataAugmentation   | 0     
2 | sigmoid       | Sigmoid            | 0     
3 | cross_entropy | BCEWithLogitsLoss  | 0     
4 | accuracy1     | MultilabelAccuracy | 0     
5 | F1_M          | MultilabelF1Score  | 0     
6 | F1_m          | MultilabelF1Score  | 0     
7 | F1_N          | MultilabelF1Score  | 0     
-----------------------------------------------------
20.2 M    Trainable params
0         Non-trainable params
20.2 M    Total params
80.730    Total estimated model params size (MB)
`Trainer.fit` stopped: `max_epochs=60` reached.
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                        LR â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–â–â–â–â–
wandb:                     epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:            train_F1_Macro â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–…â–‚â–†â–ƒâ–‚â–†â–„â–ƒâ–„â–„â–‡â–ƒâ–…â–„â–…â–†â–†â–…â–†â–†â–ƒâ–ˆâ–ƒâ–‡â–…â–…â–„â–†â–…â–„â–ƒâ–…â–‡
wandb:            train_F1_micro â–â–‚â–â–‚â–‚â–ƒâ–„â–„â–‚â–„â–ƒâ–ƒâ–†â–…â–ƒâ–„â–…â–†â–„â–„â–…â–…â–„â–…â–ˆâ–†â–…â–†â–ˆâ–„â–†â–…â–„â–„â–†â–…â–„â–†â–…â–‡
wandb:           train_acc_micro â–â–‚â–‚â–‚â–ƒâ–ƒâ–…â–…â–‚â–…â–„â–„â–†â–†â–„â–…â–†â–†â–…â–…â–†â–†â–…â–†â–ˆâ–‡â–†â–†â–ˆâ–…â–‡â–†â–…â–…â–‡â–†â–…â–†â–†â–‡
wandb:         train_f1_big tits â–„â–â–â–ƒâ–…â–ƒâ–‡â–…â–…â–‚â–ƒâ–ˆâ–„â–…â–…â–…â–†â–…â–‚â–…â–ˆâ–†â–„â–…â–‡â–†â–†â–‡â–†â–†â–†â–„â–…â–„â–‡â–ˆâ–†â–‡â–†â–‡
wandb:     train_f1_flat chested â–‚â–…â–‚â–ƒâ–„â–„â–…â–„â–ƒâ–†â–…â–â–†â–…â–ˆâ–…â–†â–†â–‡â–†â–‡â–‡â–‡â–‡â–…â–‡â–‡â–â–ˆâ–ƒâ–†â–†â–‡â–„â–†â–…â–…â–‚â–ˆâ–‡
wandb:        train_f1_huge tits â–„â–â–„â–„â–…â–…â–…â–ˆâ–ƒâ–ˆâ–â–ƒâ–„â–â–â–ƒâ–â–‡â–â–„â–â–â–…â–…â–â–„â–…â–â–…â–ƒâ–†â–ƒâ–ƒâ–…â–„â–…â–â–â–ƒâ–„
wandb:       train_f1_small tits â–â–„â–…â–ƒâ–‚â–„â–â–ƒâ–‚â–…â–…â–‚â–†â–†â–â–†â–…â–‡â–…â–„â–ƒâ–…â–…â–…â–‡â–†â–„â–‡â–ˆâ–„â–…â–‡â–…â–†â–„â–ƒâ–…â–‡â–„â–‡
wandb:                train_loss â–†â–„â–ˆâ–…â–…â–‡â–ƒâ–ƒâ–„â–…â–ƒâ–„â–ƒâ–‚â–…â–„â–ƒâ–‚â–ƒâ–„â–‚â–ƒâ–„â–„â–â–‚â–‚â–‚â–â–ƒâ–‚â–‚â–ƒâ–†â–ƒâ–‚â–‚â–‚â–ƒâ–
wandb:       trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:        val_F1_Macro_epoch â–â–‚â–ƒâ–‚â–„â–ƒâ–„â–„â–„â–„â–…â–…â–†â–†â–†â–†â–‡â–†â–‡â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:         val_F1_Macro_step â–â–‚â–ƒâ–‚â–†â–„â–…â–ƒâ–ƒâ–…â–…â–„â–†â–„â–…â–…â–…â–ƒâ–‡â–†â–ˆâ–„â–‡â–„â–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–…â–ˆâ–ˆâ–…â–…
wandb:        val_F1_micro_epoch â–â–‚â–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:         val_F1_micro_step â–â–‚â–ƒâ–ƒâ–…â–ƒâ–„â–ƒâ–…â–…â–…â–‡â–†â–†â–…â–†â–‡â–†â–†â–†â–ˆâ–‡â–ˆâ–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆ
wandb:       val_acc_micro_epoch â–â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:        val_acc_micro_step â–â–‚â–ƒâ–„â–†â–„â–…â–ƒâ–†â–†â–†â–‡â–‡â–‡â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–‡â–ˆâ–‡â–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:     val_f1_big tits_epoch â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–„â–…â–…â–†â–†â–†â–‡â–†â–†â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:      val_f1_big tits_step â–ƒâ–‚â–â–…â–…â–‡â–†â–ƒâ–†â–„â–…â–‡â–†â–†â–†â–†â–‡â–‡â–‡â–…â–ˆâ–‡â–ˆâ–ˆâ–‡â–ˆâ–‡â–ˆâ–‡â–ˆâ–ˆâ–‡â–‡â–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆ
wandb: val_f1_flat chested_epoch â–â–â–ƒâ–‚â–„â–„â–„â–…â–…â–„â–…â–…â–†â–†â–†â–†â–†â–…â–‡â–†â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:  val_f1_flat chested_step â–‚â–â–…â–…â–†â–„â–†â–ƒâ–‡â–…â–‡â–‡â–‡â–‡â–‡â–†â–ˆâ–…â–‡â–ˆâ–ˆâ–‡â–ˆâ–‡â–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆ
wandb:    val_f1_huge tits_epoch â–â–â–ƒâ–‚â–…â–ƒâ–„â–„â–„â–„â–…â–†â–†â–†â–†â–†â–‡â–†â–‡â–‡â–ˆâ–†â–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:     val_f1_huge tits_step â–â–„â–„â–â–‡â–…â–†â–…â–â–†â–…â–â–†â–â–…â–…â–â–â–ˆâ–†â–ˆâ–â–†â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–â–ˆâ–ˆâ–â–
wandb:   val_f1_small tits_epoch â–â–‚â–ƒâ–„â–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–…â–†â–†â–†â–†â–‡â–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–‡â–ˆâ–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    val_f1_small tits_step â–ƒâ–„â–„â–‚â–ƒâ–â–‚â–ƒâ–„â–†â–ƒâ–†â–†â–†â–ƒâ–†â–‡â–†â–…â–ˆâ–‡â–†â–ˆâ–†â–‡â–ˆâ–‡â–ˆâ–ˆâ–†â–ˆâ–ˆâ–ˆâ–†â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆ
wandb:            val_loss_epoch â–ˆâ–‡â–†â–†â–…â–…â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:             val_loss_step â–…â–†â–„â–ˆâ–ƒâ–„â–ƒâ–„â–ƒâ–ƒâ–„â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–‚â–â–â–â–â–â–â–â–‚â–â–‚â–â–â–
wandb: 
wandb: Run summary:
wandb:                        LR 3e-05
wandb:                     epoch 59
wandb:            train_F1_Macro 0.58889
wandb:            train_F1_micro 0.64516
wandb:           train_acc_micro 0.725
wandb:         train_f1_big tits 0.66667
wandb:     train_f1_flat chested 0.66667
wandb:        train_f1_huge tits 0.22222
wandb:       train_f1_small tits 0.8
wandb:                train_loss 73.53866
wandb:       trainer/global_step 47579
wandb:        val_F1_Macro_epoch 0.91526
wandb:         val_F1_Macro_step 0.71875
wandb:        val_F1_micro_epoch 0.98371
wandb:         val_F1_micro_step 0.95238
wandb:       val_acc_micro_epoch 0.99161
wandb:        val_acc_micro_step 0.975
wandb:     val_f1_big tits_epoch 0.98613
wandb:      val_f1_big tits_step 1.0
wandb: val_f1_flat chested_epoch 0.98279
wandb:  val_f1_flat chested_step 1.0
wandb:    val_f1_huge tits_epoch 0.71248
wandb:     val_f1_huge tits_step 0.0
wandb:   val_f1_small tits_epoch 0.97964
wandb:    val_f1_small tits_step 0.875
wandb:            val_loss_epoch 5.45758
wandb:             val_loss_step 4.93269
wandb: 
wandb: ğŸš€ View run version_2_all_eff_36_0.001 at: https://wandb.ai/tsu/tits_size/runs/version_2_all_eff_36_0.001
wandb: Synced 6 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)
wandb: Find logs at: /home/timssh/ML/TAGGING/DATA/models/wandb/run-20230716_163151-version_2_all_eff_36_0.001/logs
Global seed set to 1
/home/timssh/miniconda3/envs/tagging/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:197: UserWarning: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.
  rank_zero_warn(
wandb: Currently logged in as: timasaviin (tsu). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.15.5
wandb: Run data is saved locally in /home/timssh/ML/TAGGING/DATA/models/wandb/run-20230717_004243-version_0_all_eff_36_0.001
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run version_0_all_eff_36_0.001
wandb: â­ï¸ View project at https://wandb.ai/tsu/hair_type
wandb: ğŸš€ View run at https://wandb.ai/tsu/hair_type/runs/version_0_all_eff_36_0.001
/home/timssh/miniconda3/envs/tagging/lib/python3.10/site-packages/lightning_fabric/connector.py:562: UserWarning: 16 is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!
  rank_zero_warn(
Using 16bit Automatic Mixed Precision (AMP)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]

  | Name          | Type               | Params
-----------------------------------------------------
0 | model         | EfficientNet       | 20.2 M
1 | transform     | DataAugmentation   | 0     
2 | sigmoid       | Sigmoid            | 0     
3 | cross_entropy | BCEWithLogitsLoss  | 0     
4 | accuracy1     | MultilabelAccuracy | 0     
5 | F1_M          | MultilabelF1Score  | 0     
6 | F1_m          | MultilabelF1Score  | 0     
7 | F1_N          | MultilabelF1Score  | 0     
-----------------------------------------------------
20.2 M    Trainable params
0         Non-trainable params
20.2 M    Total params
80.725    Total estimated model params size (MB)
`Trainer.fit` stopped: `max_epochs=60` reached.
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                         LR â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–â–â–â–â–
wandb:                      epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:             train_F1_Macro â–â–…â–…â–„â–„â–…â–ƒâ–„â–„â–…â–ƒâ–†â–…â–â–…â–„â–ƒâ–„â–„â–‡â–†â–†â–…â–†â–‡â–…â–„â–ˆâ–‡â–‡â–‚â–„â–…â–‡â–†â–†â–†â–‡â–ˆâ–…
wandb:             train_F1_micro â–â–‚â–‚â–â–‚â–„â–â–‚â–‚â–‚â–‚â–„â–„â–ƒâ–„â–‚â–„â–‚â–ˆâ–…â–„â–„â–„â–†â–ˆâ–†â–…â–…â–…â–…â–‚â–ƒâ–„â–„â–†â–…â–†â–…â–ˆâ–†
wandb:            train_acc_micro â–‚â–ƒâ–‚â–â–ƒâ–„â–â–‚â–‚â–ƒâ–‚â–…â–„â–„â–„â–‚â–…â–‚â–ˆâ–†â–„â–„â–…â–†â–ˆâ–†â–…â–†â–†â–†â–‚â–ƒâ–„â–„â–†â–…â–†â–†â–ˆâ–†
wandb:      train_f1_curly haired â–â–„â–†â–„â–ƒâ–…â–‚â–…â–…â–†â–ƒâ–…â–ƒâ–â–…â–…â–â–„â–â–†â–…â–ˆâ–…â–„â–…â–„â–â–ˆâ–…â–ˆâ–‚â–â–ƒâ–‡â–ƒâ–…â–„â–‡â–†â–ƒ
wandb:     train_f1_straight hair â–„â–ƒâ–ƒâ–„â–ƒâ–…â–ƒâ–„â–â–ƒâ–„â–‚â–†â–‡â–…â–…â–„â–…â–‡â–‚â–„â–…â–†â–…â–†â–ˆâ–‡â–‚â–ƒâ–†â–†â–„â–„â–…â–…â–†â–‡â–…â–‡â–†
wandb:         train_f1_wavy hair â–ƒâ–…â–„â–„â–†â–…â–…â–„â–…â–„â–„â–‡â–…â–â–„â–‚â–†â–ƒâ–‡â–ˆâ–†â–„â–…â–‡â–‡â–„â–†â–‡â–ˆâ–…â–‚â–†â–†â–†â–‡â–…â–†â–†â–‡â–‡
wandb:                 train_loss â–„â–†â–…â–…â–„â–„â–„â–…â–ƒâ–†â–…â–‚â–„â–ƒâ–‡â–†â–‚â–„â–â–‚â–…â–ˆâ–ƒâ–‚â–â–‚â–‚â–†â–„â–‚â–ƒâ–ƒâ–…â–‚â–‚â–ƒâ–â–…â–â–„
wandb:        trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:         val_F1_Macro_epoch â–‚â–â–‚â–ƒâ–ƒâ–‚â–…â–„â–ƒâ–…â–„â–„â–†â–†â–…â–…â–†â–†â–†â–‡â–†â–‡â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:          val_F1_Macro_step â–†â–„â–„â–…â–‚â–ƒâ–†â–â–†â–„â–…â–„â–†â–‡â–†â–…â–„â–‡â–‡â–ˆâ–‚â–‡â–†â–‡â–ˆâ–ˆâ–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‚â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:         val_F1_micro_epoch â–â–â–‚â–ƒâ–ƒâ–‚â–„â–„â–ƒâ–…â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–†â–†â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:          val_F1_micro_step â–‚â–â–‚â–„â–‚â–‚â–ƒâ–ƒâ–„â–„â–ƒâ–„â–„â–…â–…â–ƒâ–‚â–…â–†â–ˆâ–‡â–†â–…â–†â–‡â–ˆâ–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:        val_acc_micro_epoch â–â–â–ƒâ–„â–ƒâ–ƒâ–…â–„â–„â–…â–…â–„â–…â–†â–†â–…â–†â–†â–†â–‡â–‡â–‡â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:         val_acc_micro_step â–‚â–â–‚â–„â–‚â–‚â–„â–ƒâ–„â–…â–„â–„â–„â–…â–…â–„â–‚â–…â–‡â–ˆâ–‡â–‡â–…â–‡â–‡â–ˆâ–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:  val_f1_curly haired_epoch â–ƒâ–â–ƒâ–ƒâ–‚â–‚â–…â–…â–„â–†â–ƒâ–„â–‡â–‡â–†â–†â–‡â–†â–†â–ˆâ–†â–‡â–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:   val_f1_curly haired_step â–ˆâ–†â–†â–‡â–ƒâ–…â–ˆâ–â–ˆâ–„â–‡â–…â–ˆâ–ˆâ–‡â–†â–†â–ˆâ–ˆâ–ˆâ–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb: val_f1_straight hair_epoch â–â–‚â–â–ƒâ–ƒâ–ƒâ–„â–‚â–„â–…â–…â–„â–…â–„â–…â–†â–„â–…â–†â–†â–†â–†â–…â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:  val_f1_straight hair_step â–ƒâ–ƒâ–â–‡â–†â–ˆâ–„â–‚â–†â–…â–…â–‡â–ƒâ–…â–ˆâ–„â–‚â–…â–ˆâ–‡â–†â–†â–‡â–†â–‡â–‡â–ˆâ–ˆâ–‡â–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:     val_f1_wavy hair_epoch â–â–â–ƒâ–„â–ƒâ–‚â–„â–…â–ƒâ–„â–„â–„â–„â–†â–…â–ƒâ–†â–†â–…â–†â–†â–‡â–†â–†â–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:      val_f1_wavy hair_step â–„â–ƒâ–…â–„â–„â–â–…â–†â–„â–‡â–…â–„â–†â–†â–…â–…â–†â–†â–…â–ˆâ–ˆâ–‡â–„â–‡â–‡â–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆ
wandb:             val_loss_epoch â–‡â–ˆâ–†â–†â–…â–†â–„â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:              val_loss_step â–†â–‡â–†â–„â–‡â–†â–†â–‡â–ˆâ–ƒâ–„â–…â–ƒâ–‚â–‚â–„â–ƒâ–†â–â–‚â–‚â–‚â–ƒâ–ƒâ–‚â–â–â–‚â–â–â–â–â–â–â–â–„â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                         LR 3e-05
wandb:                      epoch 59
wandb:             train_F1_Macro 0.72415
wandb:             train_F1_micro 0.80769
wandb:            train_acc_micro 0.84848
wandb:      train_f1_curly haired 0.5
wandb:     train_f1_straight hair 0.90323
wandb:         train_f1_wavy hair 0.76923
wandb:                 train_loss 37.17967
wandb:        trainer/global_step 54359
wandb:         val_F1_Macro_epoch 0.92315
wandb:          val_F1_Macro_step 1.0
wandb:         val_F1_micro_epoch 0.99104
wandb:          val_F1_micro_step 1.0
wandb:        val_acc_micro_epoch 0.9939
wandb:         val_acc_micro_step 1.0
wandb:  val_f1_curly haired_epoch 0.79234
wandb:   val_f1_curly haired_step 1.0
wandb: val_f1_straight hair_epoch 0.99427
wandb:  val_f1_straight hair_step 1.0
wandb:     val_f1_wavy hair_epoch 0.98285
wandb:      val_f1_wavy hair_step 1.0
wandb:             val_loss_epoch 2.73898
wandb:              val_loss_step 0.06204
wandb: 
wandb: ğŸš€ View run version_0_all_eff_36_0.001 at: https://wandb.ai/tsu/hair_type/runs/version_0_all_eff_36_0.001
wandb: Synced 6 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)
wandb: Find logs at: /home/timssh/ML/TAGGING/DATA/models/wandb/run-20230717_004243-version_0_all_eff_36_0.001/logs
Global seed set to 1
/home/timssh/miniconda3/envs/tagging/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:197: UserWarning: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.
  rank_zero_warn(
wandb: Currently logged in as: timasaviin (tsu). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.15.5
wandb: Run data is saved locally in /home/timssh/ML/TAGGING/DATA/models/wandb/run-20230717_100143-version_0_all_eff_36_0.001
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run version_0_all_eff_36_0.001
wandb: â­ï¸ View project at https://wandb.ai/tsu/sex_positions
wandb: ğŸš€ View run at https://wandb.ai/tsu/sex_positions/runs/version_0_all_eff_36_0.001
/home/timssh/miniconda3/envs/tagging/lib/python3.10/site-packages/lightning_fabric/connector.py:562: UserWarning: 16 is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!
  rank_zero_warn(
Using 16bit Automatic Mixed Precision (AMP)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]

  | Name          | Type               | Params
-----------------------------------------------------
0 | model         | EfficientNet       | 20.2 M
1 | transform     | DataAugmentation   | 0     
2 | sigmoid       | Sigmoid            | 0     
3 | cross_entropy | BCEWithLogitsLoss  | 0     
4 | accuracy1     | MultilabelAccuracy | 0     
5 | F1_M          | MultilabelF1Score  | 0     
6 | F1_m          | MultilabelF1Score  | 0     
7 | F1_N          | MultilabelF1Score  | 0     
-----------------------------------------------------
20.2 M    Trainable params
0         Non-trainable params
20.2 M    Total params
80.741    Total estimated model params size (MB)
`Trainer.fit` stopped: `max_epochs=60` reached.
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                           LR â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–„â–„â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:                        epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:               train_F1_Macro â–‚â–ƒâ–…â–…â–„â–‡â–„â–„â–â–ƒâ–†â–…â–â–‚â–‚â–‚â–ˆâ–…â–â–†â–ƒâ–ƒâ–…â–ƒâ–„â–‡â–…â–†â–†â–â–‡â–â–†â–ƒâ–†â–ƒâ–‚â–‚â–…â–„
wandb:               train_F1_micro â–â–ƒâ–…â–…â–„â–†â–„â–„â–â–ƒâ–…â–…â–‚â–‚â–…â–‚â–ˆâ–…â–â–†â–„â–ƒâ–…â–‚â–ƒâ–ˆâ–„â–†â–…â–‚â–‡â–‚â–…â–ƒâ–…â–ƒâ–‚â–‚â–…â–„
wandb:              train_acc_micro â–â–ƒâ–…â–†â–„â–‡â–…â–…â–â–„â–†â–…â–ƒâ–‚â–†â–‚â–ˆâ–…â–‚â–‡â–„â–„â–…â–ƒâ–„â–ˆâ–…â–‡â–†â–‚â–‡â–‚â–…â–„â–†â–ƒâ–ƒâ–‚â–…â–…
wandb:                  train_f1_69 â–…â–…â–ˆâ–†â–„â–†â–†â–…â–„â–ƒâ–…â–…â–…â–†â–â–„â–ˆâ–†â–…â–„â–„â–…â–†â–ƒâ–†â–…â–ƒâ–†â–‡â–„â–†â–…â–†â–ƒâ–„â–…â–ƒâ–…â–†â–…
wandb:             train_f1_cowgirl â–‚â–…â–â–‚â–…â–†â–ˆâ–…â–†â–†â–„â–…â–ƒâ–‚â–…â–…â–†â–†â–‚â–‡â–„â–†â–†â–…â–…â–‡â–†â–…â–…â–‚â–ƒâ–…â–‡â–…â–‡â–…â–„â–†â–‡â–†
wandb:         train_f1_doggy style â–â–ƒâ–†â–†â–†â–„â–…â–„â–ƒâ–†â–†â–„â–†â–†â–†â–ƒâ–ˆâ–†â–â–…â–‡â–ƒâ–„â–…â–…â–ƒâ–…â–†â–„â–„â–ˆâ–„â–…â–„â–…â–‚â–…â–ƒâ–„â–…
wandb:          train_f1_missionary â–„â–ƒâ–…â–ƒâ–…â–†â–„â–„â–ƒâ–†â–‡â–„â–‚â–„â–‚â–‚â–ƒâ–„â–ƒâ–†â–ƒâ–ƒâ–†â–…â–„â–ˆâ–…â–‚â–…â–ƒâ–‡â–â–„â–ƒâ–…â–‚â–…â–‚â–ƒâ–„
wandb:     train_f1_reverse cowgirl â–„â–„â–„â–…â–„â–ˆâ–‚â–ˆâ–â–†â–‡â–…â–‚â–‚â–‡â–…â–…â–‚â–…â–„â–‚â–„â–‚â–„â–…â–†â–…â–ˆâ–„â–„â–†â–ƒâ–†â–†â–†â–…â–„â–…â–‚â–‚
wandb:            train_f1_spooning â–‡â–‡â–†â–‡â–…â–†â–‚â–„â–…â–â–†â–‡â–„â–…â–ˆâ–…â–†â–†â–†â–ˆâ–‡â–†â–‡â–…â–…â–ˆâ–…â–†â–ˆâ–†â–†â–…â–…â–†â–…â–†â–…â–ƒâ–‡â–†
wandb:                   train_loss â–ˆâ–‡â–„â–„â–…â–ƒâ–…â–…â–ˆâ–…â–ƒâ–„â–‡â–†â–ƒâ–‡â–â–„â–‡â–ƒâ–„â–…â–„â–‡â–†â–‚â–…â–‚â–ƒâ–‡â–‚â–‡â–„â–…â–„â–†â–‡â–ˆâ–„â–…
wandb:          trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:           val_F1_Macro_epoch â–â–ƒâ–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:            val_F1_Macro_step â–â–ƒâ–…â–…â–‡â–‡â–‡â–…â–‡â–†â–ˆâ–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:           val_F1_micro_epoch â–â–ƒâ–†â–†â–‡â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:            val_F1_micro_step â–â–ƒâ–„â–„â–†â–†â–ˆâ–†â–‡â–‡â–ˆâ–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:          val_acc_micro_epoch â–â–ƒâ–†â–†â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:           val_acc_micro_step â–â–ƒâ–„â–…â–‡â–‡â–ˆâ–†â–‡â–‡â–ˆâ–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:              val_f1_69_epoch â–â–â–†â–‡â–ˆâ–†â–‡â–ˆâ–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:               val_f1_69_step â–„â–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–…â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:         val_f1_cowgirl_epoch â–â–‚â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–†â–‡â–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:          val_f1_cowgirl_step â–‚â–â–†â–‡â–‡â–ˆâ–ˆâ–‚â–‡â–ˆâ–ˆâ–‡â–†â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆ
wandb:     val_f1_doggy style_epoch â–â–ƒâ–†â–‡â–†â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:      val_f1_doggy style_step â–…â–ˆâ–ƒâ–„â–â–ƒâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:      val_f1_missionary_epoch â–â–…â–‡â–†â–‡â–†â–‡â–‡â–ˆâ–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:       val_f1_missionary_step â–â–‡â–…â–ˆâ–ˆâ–‡â–†â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: val_f1_reverse cowgirl_epoch â–â–â–„â–„â–…â–†â–†â–‡â–‡â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:  val_f1_reverse cowgirl_step â–„â–…â–…â–â–‡â–‡â–ˆâ–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:        val_f1_spooning_epoch â–â–†â–…â–ƒâ–‡â–†â–‡â–‡â–ˆâ–„â–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:         val_f1_spooning_step â–ˆâ–„â–ˆâ–ˆâ–ˆâ–…â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:               val_loss_epoch â–ˆâ–†â–ƒâ–ƒâ–‚â–‚â–‚â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                val_loss_step â–ˆâ–‡â–ƒâ–„â–‚â–‚â–â–„â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–‚â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–‚â–
wandb: 
wandb: Run summary:
wandb:                           LR 1e-05
wandb:                        epoch 59
wandb:               train_F1_Macro 0.61538
wandb:               train_F1_micro 0.74194
wandb:              train_acc_micro 0.88406
wandb:                  train_f1_69 0.66667
wandb:             train_f1_cowgirl 0.5
wandb:         train_f1_doggy style 0.72727
wandb:          train_f1_missionary 0.0
wandb:     train_f1_reverse cowgirl 0.85714
wandb:            train_f1_spooning 0.94118
wandb:                   train_loss 46.51409
wandb:          trainer/global_step 26279
wandb:           val_F1_Macro_epoch 0.99048
wandb:            val_F1_Macro_step 1.0
wandb:           val_F1_micro_epoch 0.99925
wandb:            val_F1_micro_step 1.0
wandb:          val_acc_micro_epoch 0.99975
wandb:           val_acc_micro_step 1.0
wandb:              val_f1_69_epoch 0.9863
wandb:               val_f1_69_step 1.0
wandb:         val_f1_cowgirl_epoch 0.99506
wandb:          val_f1_cowgirl_step 1.0
wandb:     val_f1_doggy style_epoch 1.0
wandb:      val_f1_doggy style_step 1.0
wandb:      val_f1_missionary_epoch 0.96347
wandb:       val_f1_missionary_step 1.0
wandb: val_f1_reverse cowgirl_epoch 0.99807
wandb:  val_f1_reverse cowgirl_step 1.0
wandb:        val_f1_spooning_epoch 1.0
wandb:         val_f1_spooning_step 1.0
wandb:               val_loss_epoch 0.32201
wandb:                val_loss_step 0.00575
wandb: 
wandb: ğŸš€ View run version_0_all_eff_36_0.001 at: https://wandb.ai/tsu/sex_positions/runs/version_0_all_eff_36_0.001
wandb: Synced 6 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)
wandb: Find logs at: /home/timssh/ML/TAGGING/DATA/models/wandb/run-20230717_100143-version_0_all_eff_36_0.001/logs
Global seed set to 1
/home/timssh/miniconda3/envs/tagging/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:197: UserWarning: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.
  rank_zero_warn(
wandb: Currently logged in as: timasaviin (tsu). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.15.5
wandb: Run data is saved locally in /home/timssh/ML/TAGGING/DATA/models/wandb/run-20230717_140108-version_0_all_eff_36_0.001
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run version_0_all_eff_36_0.001
wandb: â­ï¸ View project at https://wandb.ai/tsu/hair_color
wandb: ğŸš€ View run at https://wandb.ai/tsu/hair_color/runs/version_0_all_eff_36_0.001
/home/timssh/miniconda3/envs/tagging/lib/python3.10/site-packages/lightning_fabric/connector.py:562: UserWarning: 16 is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!
  rank_zero_warn(
Using 16bit Automatic Mixed Precision (AMP)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]

  | Name          | Type               | Params
-----------------------------------------------------
0 | model         | EfficientNet       | 20.2 M
1 | transform     | DataAugmentation   | 0     
2 | sigmoid       | Sigmoid            | 0     
3 | cross_entropy | BCEWithLogitsLoss  | 0     
4 | accuracy1     | MultilabelAccuracy | 0     
5 | F1_M          | MultilabelF1Score  | 0     
6 | F1_m          | MultilabelF1Score  | 0     
7 | F1_N          | MultilabelF1Score  | 0     
-----------------------------------------------------
20.2 M    Trainable params
0         Non-trainable params
20.2 M    Total params
80.751    Total estimated model params size (MB)
`Trainer.fit` stopped: `max_epochs=60` reached.
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                       LR â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–„â–„â–„â–„â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–
wandb:                    epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:           train_F1_Macro â–â–‚â–„â–ƒâ–‚â–„â–…â–ƒâ–‡â–„â–†â–…â–ƒâ–â–„â–ƒâ–ƒâ–…â–„â–„â–…â–„â–ƒâ–ƒâ–ˆâ–„â–ƒâ–ƒâ–ƒâ–ˆâ–†â–‚â–†â–ƒâ–…â–‚â–ƒâ–‚â–ˆâ–„
wandb:           train_F1_micro â–‚â–‚â–ƒâ–„â–â–ƒâ–„â–„â–„â–„â–†â–„â–ƒâ–â–„â–„â–„â–„â–„â–„â–„â–„â–ƒâ–‚â–‡â–ƒâ–ƒâ–ƒâ–‚â–ˆâ–†â–‚â–„â–„â–ƒâ–ƒâ–‚â–„â–„â–ƒ
wandb:          train_acc_micro â–ƒâ–ƒâ–„â–…â–â–„â–…â–…â–…â–…â–‡â–…â–„â–‚â–…â–…â–…â–…â–…â–…â–…â–…â–ƒâ–ƒâ–ˆâ–„â–„â–„â–ƒâ–ˆâ–‡â–‚â–…â–…â–…â–„â–ƒâ–…â–†â–…
wandb:      train_f1_black hair â–†â–†â–„â–‡â–ƒâ–â–…â–ˆâ–…â–…â–…â–…â–†â–„â–…â–‡â–‡â–…â–†â–ƒâ–‡â–…â–„â–†â–†â–…â–…â–…â–…â–‡â–…â–†â–„â–†â–†â–‡â–„â–†â–†â–†
wandb:     train_f1_blonde hair â–â–„â–†â–…â–‚â–†â–‡â–ƒâ–ƒâ–†â–ˆâ–…â–ƒâ–„â–„â–…â–…â–„â–„â–‡â–…â–†â–†â–‚â–ˆâ–…â–…â–…â–‚â–‡â–ˆâ–„â–†â–„â–ƒâ–…â–„â–ƒâ–…â–ƒ
wandb:       train_f1_blue hair â–â–â–…â–â–ƒâ–†â–â–â–‡â–â–â–â–„â–â–‡â–â–â–„â–„â–„â–†â–„â–…â–‡â–†â–†â–ƒâ–â–â–‡â–â–â–„â–â–†â–â–ƒâ–â–ˆâ–„
wandb:      train_f1_brown hair â–…â–‚â–„â–„â–…â–…â–…â–â–…â–ƒâ–„â–…â–„â–…â–†â–…â–…â–…â–†â–‡â–‚â–…â–…â–‚â–†â–…â–†â–‡â–†â–ˆâ–†â–„â–†â–ˆâ–…â–†â–†â–†â–„â–„
wandb:      train_f1_green hair â–â–â–â–â–†â–…â–…â–…â–ˆâ–â–â–…â–…â–â–…â–â–â–â–…â–â–…â–â–â–„â–â–â–„â–â–â–â–â–â–â–…â–â–â–â–â–…â–…
wandb:       train_f1_pink hair â–â–â–„â–â–‚â–…â–…â–ƒâ–†â–ƒâ–†â–†â–â–‚â–â–ƒâ–â–…â–„â–…â–ƒâ–ƒâ–â–â–„â–ƒâ–ƒâ–â–„â–ˆâ–„â–â–„â–â–„â–â–â–ƒâ–…â–ƒ
wandb:        train_f1_red hair â–„â–…â–„â–†â–‚â–ƒâ–ƒâ–†â–†â–ˆâ–†â–…â–„â–‚â–ƒâ–ƒâ–†â–ˆâ–â–ƒâ–…â–„â–„â–†â–…â–„â–â–…â–„â–â–ˆâ–„â–…â–ƒâ–…â–ƒâ–…â–â–‡â–„
wandb:     train_f1_violet hair â–â–â–â–â–ƒâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–ˆâ–â–â–â–†â–â–â–„â–…â–â–â–â–„â–…â–â–
wandb:               train_loss â–‚â–‚â–ƒâ–‚â–ˆâ–„â–â–‚â–ƒâ–‚â–â–‚â–‚â–‚â–â–â–‚â–â–ƒâ–‚â–„â–‚â–‚â–„â–â–‚â–‚â–â–‚â–â–â–‚â–‡â–â–‚â–â–ƒâ–†â–‚â–„
wandb:      trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:       val_F1_Macro_epoch â–â–‚â–‚â–„â–ƒâ–ƒâ–ƒâ–„â–ƒâ–„â–„â–ƒâ–…â–…â–„â–…â–…â–†â–†â–†â–‡â–‡â–‡â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:        val_F1_Macro_step â–„â–‚â–â–â–ƒâ–„â–„â–„â–ƒâ–‚â–ƒâ–‚â–†â–‚â–†â–ƒâ–…â–†â–†â–†â–ƒâ–…â–‚â–…â–‚â–„â–†â–„â–†â–ƒâ–†â–„â–„â–†â–„â–„â–ƒâ–‚â–†â–ˆ
wandb:       val_F1_micro_epoch â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–ƒâ–„â–„â–ƒâ–„â–„â–„â–…â–…â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:        val_F1_micro_step â–â–‚â–„â–„â–…â–‚â–ƒâ–„â–â–„â–…â–‚â–ƒâ–ƒâ–„â–„â–„â–†â–†â–†â–…â–†â–†â–†â–‡â–‡â–†â–‡â–‡â–†â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–ˆâ–‡
wandb:      val_acc_micro_epoch â–â–‚â–ƒâ–„â–„â–ƒâ–ƒâ–„â–„â–…â–…â–ƒâ–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:       val_acc_micro_step â–â–‚â–…â–„â–…â–‚â–ƒâ–…â–â–…â–…â–ƒâ–ƒâ–„â–„â–…â–…â–‡â–‡â–‡â–†â–‡â–†â–‡â–‡â–ˆâ–‡â–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:  val_f1_black hair_epoch â–â–‚â–ƒâ–ƒâ–„â–ƒâ–ƒâ–ƒâ–„â–‚â–„â–ƒâ–„â–„â–„â–ƒâ–…â–…â–…â–†â–…â–†â–†â–†â–†â–‡â–†â–‡â–ˆâ–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:   val_f1_black hair_step â–â–†â–…â–„â–ˆâ–…â–†â–‡â–ƒâ–†â–…â–„â–„â–†â–…â–‡â–‡â–†â–‡â–…â–…â–‡â–„â–ˆâ–‡â–ˆâ–†â–‡â–†â–†â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–ˆâ–‡
wandb: val_f1_blonde hair_epoch â–‚â–‚â–ƒâ–‚â–ƒâ–„â–„â–ƒâ–„â–„â–„â–â–ƒâ–„â–†â–…â–„â–…â–‡â–…â–‡â–†â–†â–…â–ˆâ–ˆâ–‡â–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:  val_f1_blonde hair_step â–‚â–ƒâ–…â–†â–ˆâ–ƒâ–…â–â–„â–‡â–ˆâ–ƒâ–‚â–„â–…â–„â–„â–…â–‡â–ˆâ–ˆâ–…â–ˆâ–„â–ˆâ–†â–†â–†â–„â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–„â–†â–ˆâ–ˆâ–ˆâ–ˆ
wandb:   val_f1_blue hair_epoch â–â–†â–†â–‡â–‡â–‡â–†â–†â–†â–†â–‡â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–ˆâ–‡â–ˆâ–‡â–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    val_f1_blue hair_step â–ˆâ–ˆâ–â–â–â–‡â–â–†â–‡â–â–ˆâ–ˆâ–†â–ˆâ–ˆâ–â–ˆâ–ˆâ–ˆâ–ˆâ–â–ˆâ–â–â–â–â–ˆâ–â–ˆâ–ˆâ–ˆâ–ˆâ–â–â–ˆâ–ˆâ–ˆâ–â–â–
wandb:  val_f1_brown hair_epoch â–â–‚â–‚â–‚â–‚â–â–‚â–ƒâ–‚â–ƒâ–ƒâ–‚â–ƒâ–ƒâ–ƒâ–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:   val_f1_brown hair_step â–…â–…â–†â–…â–…â–„â–‚â–ƒâ–„â–‚â–„â–…â–…â–†â–„â–…â–â–‡â–…â–‡â–…â–‡â–‡â–†â–‡â–‡â–‡â–ˆâ–ˆâ–‡â–ˆâ–†â–ˆâ–‡â–ˆâ–‡â–‡â–ˆâ–ˆâ–‡
wandb:  val_f1_green hair_epoch â–ƒâ–â–‚â–†â–„â–„â–†â–‡â–„â–‡â–ˆâ–…â–ˆâ–ˆâ–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:   val_f1_green hair_step â–â–â–â–â–ˆâ–„â–ˆâ–â–â–â–â–â–ˆâ–â–ˆâ–â–â–ˆâ–ˆâ–â–†â–â–ˆâ–ˆâ–â–â–ˆâ–â–â–â–â–â–â–ˆâ–â–â–â–â–ˆâ–ˆ
wandb:   val_f1_pink hair_epoch â–†â–â–ƒâ–…â–â–†â–†â–†â–‚â–†â–‚â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    val_f1_pink hair_step â–ˆâ–â–ˆâ–â–â–ˆâ–â–ˆâ–„â–†â–â–â–ˆâ–â–ˆâ–ˆâ–ˆâ–â–â–ˆâ–â–‡â–â–ˆâ–â–ˆâ–â–ˆâ–ˆâ–â–ˆâ–â–ˆâ–ˆâ–â–â–â–â–â–ˆ
wandb:    val_f1_red hair_epoch â–‚â–ƒâ–ƒâ–„â–„â–ƒâ–â–„â–ƒâ–…â–„â–ƒâ–…â–…â–„â–„â–…â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:     val_f1_red hair_step â–…â–…â–â–‡â–†â–„â–ƒâ–ˆâ–…â–ˆâ–‡â–…â–‡â–ƒâ–†â–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–†â–ˆâ–ˆâ–ˆ
wandb: val_f1_violet hair_epoch â–â–…â–…â–‡â–…â–…â–…â–…â–‡â–†â–†â–…â–‡â–‡â–‡â–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:  val_f1_violet hair_step â–â–â–â–â–â–â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–ˆâ–ˆ
wandb:           val_loss_epoch â–ˆâ–…â–„â–„â–„â–„â–„â–ƒâ–„â–ƒâ–ƒâ–„â–ƒâ–ƒâ–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:            val_loss_step â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–ˆâ–‚â–ƒâ–â–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                       LR 1e-05
wandb:                    epoch 59
wandb:           train_F1_Macro 0.39168
wandb:           train_F1_micro 0.47761
wandb:          train_acc_micro 0.72656
wandb:      train_f1_black hair 0.44444
wandb:     train_f1_blonde hair 0.90909
wandb:       train_f1_blue hair 0.33333
wandb:      train_f1_brown hair 0.61538
wandb:      train_f1_green hair 0.0
wandb:       train_f1_pink hair 0.28571
wandb:        train_f1_red hair 0.54545
wandb:     train_f1_violet hair 0.0
wandb:               train_loss 171.3885
wandb:      trainer/global_step 56159
wandb:       val_F1_Macro_epoch 0.67683
wandb:        val_F1_Macro_step 0.6
wandb:       val_F1_micro_epoch 0.98292
wandb:        val_F1_micro_step 0.94118
wandb:      val_acc_micro_epoch 0.99561
wandb:       val_acc_micro_step 0.98438
wandb:  val_f1_black hair_epoch 0.98111
wandb:   val_f1_black hair_step 1.0
wandb: val_f1_blonde hair_epoch 0.99173
wandb:  val_f1_blonde hair_step 0.0
wandb:   val_f1_blue hair_epoch 0.56709
wandb:    val_f1_blue hair_step 1.0
wandb:  val_f1_brown hair_epoch 0.97412
wandb:   val_f1_brown hair_step 1.0
wandb:  val_f1_green hair_epoch 0.29217
wandb:   val_f1_green hair_step 0.0
wandb:   val_f1_pink hair_epoch 0.62764
wandb:    val_f1_pink hair_step 1.0
wandb:    val_f1_red hair_epoch 0.87821
wandb:     val_f1_red hair_step 0.8
wandb: val_f1_violet hair_epoch 0.10256
wandb:  val_f1_violet hair_step 0.0
wandb:           val_loss_epoch 5.03568
wandb:            val_loss_step 15.7856
wandb: 
wandb: ğŸš€ View run version_0_all_eff_36_0.001 at: https://wandb.ai/tsu/hair_color/runs/version_0_all_eff_36_0.001
wandb: Synced 6 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)
wandb: Find logs at: /home/timssh/ML/TAGGING/DATA/models/wandb/run-20230717_140108-version_0_all_eff_36_0.001/logs
Global seed set to 1
/home/timssh/miniconda3/envs/tagging/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:197: UserWarning: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.
  rank_zero_warn(
wandb: Currently logged in as: timasaviin (tsu). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.15.5
wandb: Run data is saved locally in /home/timssh/ML/TAGGING/DATA/models/wandb/run-20230720_080654-version_0_all_eff_36_0.001
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run version_0_all_eff_36_0.001
wandb: â­ï¸ View project at https://wandb.ai/tsu/body_type
wandb: ğŸš€ View run at https://wandb.ai/tsu/body_type/runs/version_0_all_eff_36_0.001
/home/timssh/miniconda3/envs/tagging/lib/python3.10/site-packages/lightning_fabric/connector.py:562: UserWarning: 16 is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!
  rank_zero_warn(
Using 16bit Automatic Mixed Precision (AMP)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]

  | Name          | Type               | Params
-----------------------------------------------------
0 | model         | EfficientNet       | 20.2 M
1 | transform     | DataAugmentation   | 0     
2 | sigmoid       | Sigmoid            | 0     
3 | cross_entropy | BCEWithLogitsLoss  | 0     
4 | accuracy1     | MultilabelAccuracy | 0     
5 | F1_M          | MultilabelF1Score  | 0     
6 | F1_m          | MultilabelF1Score  | 0     
7 | F1_N          | MultilabelF1Score  | 0     
-----------------------------------------------------
20.2 M    Trainable params
0         Non-trainable params
20.2 M    Total params
80.741    Total estimated model params size (MB)
