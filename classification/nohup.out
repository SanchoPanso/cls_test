Global seed set to 1
/home/timssh/miniconda3/envs/tagging/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:197: UserWarning: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.
  rank_zero_warn(
wandb: Currently logged in as: timasaviin (tsu). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.15.10
wandb: Run data is saved locally in /home/timssh/ML/TAGGING/DATA/models/wandb/run-20230911_104209-v__0_all_eff_36_0.001
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run v__0_all_eff_36_0.001
wandb: â­ï¸ View project at https://wandb.ai/tsu/tits_size
wandb: ğŸš€ View run at https://wandb.ai/tsu/tits_size/runs/v__0_all_eff_36_0.001
/home/timssh/miniconda3/envs/tagging/lib/python3.10/site-packages/lightning_fabric/connector.py:562: UserWarning: 16 is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!
  rank_zero_warn(
Using 16bit Automatic Mixed Precision (AMP)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]

  | Name          | Type               | Params
-----------------------------------------------------
0 | model         | EfficientNet       | 20.2 M
1 | transform     | DataAugmentation   | 0     
2 | sigmoid       | Sigmoid            | 0     
3 | cross_entropy | BCEWithLogitsLoss  | 0     
4 | accuracy1     | MultilabelAccuracy | 0     
5 | F1_M          | MultilabelF1Score  | 0     
6 | F1_m          | MultilabelF1Score  | 0     
7 | F1_N          | MultilabelF1Score  | 0     
-----------------------------------------------------
20.2 M    Trainable params
0         Non-trainable params
20.2 M    Total params
80.730    Total estimated model params size (MB)
Traceback (most recent call last):
  File "/home/timssh/ML/TAGGING/CLS/classification/train_wb.py", line 1, in <module>
    import torch
ModuleNotFoundError: No module named 'torch'
Global seed set to 1
/home/timssh/miniconda3/envs/tagging/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:197: UserWarning: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.
  rank_zero_warn(
wandb: Currently logged in as: timasaviin (tsu). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.15.10
wandb: Run data is saved locally in /home/timssh/ML/TAGGING/DATA/models/wandb/run-20230911_173143-v__2_all_eff_36_0.001
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run v__2_all_eff_36_0.001
wandb: â­ï¸ View project at https://wandb.ai/tsu/body_decoration_body_painting
wandb: ğŸš€ View run at https://wandb.ai/tsu/body_decoration_body_painting/runs/v__2_all_eff_36_0.001
/home/timssh/miniconda3/envs/tagging/lib/python3.10/site-packages/lightning_fabric/connector.py:562: UserWarning: 16 is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!
  rank_zero_warn(
Using 16bit Automatic Mixed Precision (AMP)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]

  | Name          | Type              | Params
----------------------------------------------------
0 | model         | EfficientNet      | 20.2 M
1 | transform     | DataAugmentation  | 0     
2 | sigmoid       | Sigmoid           | 0     
3 | cross_entropy | BCEWithLogitsLoss | 0     
4 | accuracy1     | BinaryAccuracy    | 0     
----------------------------------------------------
20.2 M    Trainable params
0         Non-trainable params
20.2 M    Total params
80.715    Total estimated model params size (MB)
`Trainer.fit` stopped: `max_epochs=60` reached.
wandb: Waiting for W&B process to finish... (success).
wandb: - 78.144 MB of 78.144 MB uploaded (0.000 MB deduped)wandb: \ 78.144 MB of 78.144 MB uploaded (0.000 MB deduped)wandb: | 78.144 MB of 78.144 MB uploaded (0.000 MB deduped)wandb: / 78.144 MB of 78.144 MB uploaded (0.000 MB deduped)wandb: - 78.144 MB of 78.144 MB uploaded (0.000 MB deduped)wandb: \ 78.144 MB of 78.144 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                        LR â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–â–â–â–â–â–â–â–â–
wandb:                     epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:            train_F1_Macro â–â–‚â–„â–â–„â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–ƒâ–„â–‚â–†â–…â–ƒâ–„â–â–†â–…â–ƒâ–…â–„â–…â–†â–„â–†â–„â–†â–†â–ƒâ–‡â–‡â–ˆâ–…â–ƒâ–„â–„
wandb:            train_F1_micro â–â–â–ƒâ–â–ƒâ–ƒâ–„â–„â–ƒâ–…â–†â–†â–ƒâ–…â–†â–…â–†â–‚â–ƒâ–„â–…â–…â–ƒâ–„â–„â–…â–‡â–„â–†â–…â–…â–†â–…â–‡â–‡â–ˆâ–†â–…â–„â–„
wandb:           train_acc_micro â–â–â–ƒâ–‚â–„â–ƒâ–…â–…â–ƒâ–…â–†â–†â–„â–…â–†â–†â–†â–„â–„â–†â–…â–†â–„â–„â–…â–†â–‡â–„â–†â–…â–…â–ˆâ–…â–‡â–†â–ˆâ–ˆâ–†â–…â–†
wandb:         train_f1_big tits â–ƒâ–â–„â–‚â–â–…â–„â–„â–…â–†â–†â–‡â–‚â–…â–ˆâ–„â–„â–‚â–‚â–ˆâ–„â–†â–…â–‚â–„â–†â–‡â–…â–…â–†â–„â–…â–‡â–ˆâ–‡â–†â–…â–‡â–…â–„
wandb:     train_f1_flat chested â–„â–„â–„â–„â–‡â–ƒâ–„â–…â–…â–…â–‡â–…â–†â–…â–ƒâ–ˆâ–„â–„â–„â–â–…â–…â–…â–†â–…â–†â–…â–…â–…â–…â–…â–…â–„â–ˆâ–†â–†â–†â–…â–…â–…
wandb:        train_f1_huge tits â–ƒâ–ƒâ–†â–â–„â–â–â–â–ƒâ–„â–…â–„â–â–â–â–„â–„â–â–ˆâ–â–†â–„â–â–†â–ƒâ–„â–â–â–„â–â–ˆâ–†â–â–â–„â–…â–â–â–â–
wandb:       train_f1_small tits â–â–„â–„â–ƒâ–ƒâ–„â–„â–…â–‚â–„â–„â–†â–…â–„â–„â–ƒâ–ˆâ–†â–…â–ƒâ–†â–†â–ƒâ–…â–†â–„â–†â–†â–‡â–…â–…â–…â–ƒâ–…â–…â–ˆâ–†â–â–…â–†
wandb:                train_loss â–„â–†â–„â–ƒâ–ƒâ–ƒâ–ƒâ–„â–†â–‚â–‚â–‚â–ƒâ–ƒâ–â–ƒâ–†â–ƒâ–„â–‚â–ƒâ–‚â–ƒâ–…â–ƒâ–ƒâ–‚â–ƒâ–ƒâ–‚â–ˆâ–ƒâ–‚â–‚â–‚â–â–â–‚â–‚â–‚
wandb:       trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:        val_F1_Macro_epoch â–â–â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–†â–…â–†â–†â–‡â–†â–‡â–‡â–†â–‡â–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:         val_F1_Macro_step â–â–â–‚â–â–„â–ƒâ–„â–ƒâ–ƒâ–ƒâ–ƒâ–‡â–„â–…â–„â–‡â–ƒâ–„â–„â–„â–†â–‡â–„â–ˆâ–„â–†â–‡â–ˆâ–ˆâ–‡â–…â–…â–…â–…â–ˆâ–„â–‡â–ˆâ–…â–ˆ
wandb:        val_F1_micro_epoch â–â–â–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–„â–…â–…â–…â–†â–…â–†â–†â–‡â–†â–‡â–‡â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:         val_F1_micro_step â–â–‚â–ƒâ–‚â–„â–„â–„â–„â–…â–…â–„â–†â–‡â–†â–†â–†â–…â–†â–†â–‡â–‡â–‡â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–†â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:       val_acc_micro_epoch â–â–â–ƒâ–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:        val_acc_micro_step â–â–‚â–„â–„â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:     val_f1_big tits_epoch â–â–â–ƒâ–ƒâ–„â–„â–„â–„â–…â–„â–„â–†â–…â–†â–†â–†â–‡â–†â–‡â–†â–‡â–‡â–†â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:      val_f1_big tits_step â–‚â–„â–„â–â–†â–†â–‡â–‡â–†â–†â–„â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: val_f1_flat chested_epoch â–‚â–â–‚â–‚â–ƒâ–ƒâ–„â–ƒâ–ƒâ–„â–†â–…â–…â–†â–†â–†â–†â–†â–…â–†â–‡â–…â–‡â–†â–‡â–‡â–‡â–ˆâ–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:  val_f1_flat chested_step â–ƒâ–‚â–…â–…â–„â–â–‡â–ƒâ–„â–†â–‡â–ˆâ–ˆâ–„â–‡â–†â–†â–ˆâ–ˆâ–ˆâ–‡â–†â–ˆâ–ˆâ–‡â–‡â–…â–‡â–ˆâ–‡â–ˆâ–ˆâ–‡â–ˆâ–ˆâ–†â–†â–ˆâ–†â–ˆ
wandb:    val_f1_huge tits_epoch â–â–â–„â–…â–„â–„â–‚â–†â–†â–…â–†â–‡â–…â–†â–„â–†â–‡â–ˆâ–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:     val_f1_huge tits_step â–â–â–â–â–…â–„â–â–â–â–â–â–ˆâ–â–†â–â–ˆâ–â–â–â–â–†â–ˆâ–â–ˆâ–â–†â–ˆâ–ˆâ–ˆâ–ˆâ–â–â–â–â–ˆâ–â–ˆâ–ˆâ–â–ˆ
wandb:   val_f1_small tits_epoch â–â–â–‚â–‚â–ƒâ–ƒâ–„â–ƒâ–„â–„â–„â–„â–…â–„â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–†â–‡â–‡â–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    val_f1_small tits_step â–‚â–â–‚â–ƒâ–‚â–„â–ƒâ–„â–„â–„â–„â–†â–ƒâ–‡â–ƒâ–†â–â–â–…â–†â–‡â–‡â–â–†â–…â–‡â–‡â–ˆâ–ˆâ–„â–‡â–ˆâ–ˆâ–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆ
wandb:            val_loss_epoch â–ˆâ–ˆâ–†â–…â–…â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:             val_loss_step â–„â–…â–„â–„â–ƒâ–ˆâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–ƒâ–â–‚â–â–‚â–â–‚â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                        LR 3e-05
wandb:                     epoch 59
wandb:            train_F1_Macro 0.4533
wandb:            train_F1_micro 0.54839
wandb:           train_acc_micro 0.70833
wandb:         train_f1_big tits 0.76923
wandb:     train_f1_flat chested 0.61538
wandb:        train_f1_huge tits 0.0
wandb:       train_f1_small tits 0.42857
wandb:                train_loss 101.49103
wandb:       trainer/global_step 39119
wandb:        val_F1_Macro_epoch 0.84378
wandb:         val_F1_Macro_step 0.75
wandb:        val_F1_micro_epoch 0.97285
wandb:         val_F1_micro_step 1.0
wandb:       val_acc_micro_epoch 0.98944
wandb:        val_acc_micro_step 1.0
wandb:     val_f1_big tits_epoch 0.98789
wandb:      val_f1_big tits_step 1.0
wandb: val_f1_flat chested_epoch 0.94875
wandb:  val_f1_flat chested_step 1.0
wandb:    val_f1_huge tits_epoch 0.48742
wandb:     val_f1_huge tits_step 0.0
wandb:   val_f1_small tits_epoch 0.95105
wandb:    val_f1_small tits_step 1.0
wandb:            val_loss_epoch 6.78946
wandb:             val_loss_step 0.09252
wandb: 
wandb: ğŸš€ View run v__0_all_eff_36_0.001 at: https://wandb.ai/tsu/tits_size/runs/v__0_all_eff_36_0.001
wandb: ï¸âš¡ View job at https://wandb.ai/tsu/tits_size/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjkwMjAxNDA5/version_details/v1
wandb: Synced 6 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)
wandb: Find logs at: /home/timssh/ML/TAGGING/DATA/models/wandb/run-20230911_104209-v__0_all_eff_36_0.001/logs
Exception in thread ChkStopThr:
Traceback (most recent call last):
  File "/home/timssh/miniconda3/envs/tagging/lib/python3.10/threading.py", line 1016, in _bootstrap_inner
    self.run()
  File "/home/timssh/miniconda3/envs/tagging/lib/python3.10/threading.py", line 953, in run
    self._target(*self._args, **self._kwargs)
  File "/home/timssh/miniconda3/envs/tagging/lib/python3.10/site-packages/wandb/sdk/wandb_run.py", line 285, in check_stop_status
    self._loop_check_status(
  File "/home/timssh/miniconda3/envs/tagging/lib/python3.10/site-packages/wandb/sdk/wandb_run.py", line 223, in _loop_check_status
    local_handle = request()
  File "/home/timssh/miniconda3/envs/tagging/lib/python3.10/site-packages/wandb/sdk/interface/interface.py", line 727, in deliver_stop_status
    return self._deliver_stop_status(status)
  File "/home/timssh/miniconda3/envs/tagging/lib/python3.10/site-packages/wandb/sdk/interface/interface_shared.py", line 450, in _deliver_stop_status
    return self._deliver_record(record)
  File "/home/timssh/miniconda3/envs/tagging/lib/python3.10/site-packages/wandb/sdk/interface/interface_shared.py", line 425, in _deliver_record
    handle = mailbox._deliver_record(record, interface=self)
  File "/home/timssh/miniconda3/envs/tagging/lib/python3.10/site-packages/wandb/sdk/lib/mailbox.py", line 455, in _deliver_record
    interface._publish(record)
  File "/home/timssh/miniconda3/envs/tagging/lib/python3.10/site-packages/wandb/sdk/interface/interface_sock.py", line 51, in _publish
    self._sock_client.send_record_publish(record)
  File "/home/timssh/miniconda3/envs/tagging/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py", line 221, in send_record_publish
    self.send_server_request(server_req)
  File "/home/timssh/miniconda3/envs/tagging/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py", line 155, in send_server_request
    self._send_message(msg)
  File "/home/timssh/miniconda3/envs/tagging/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py", line 152, in _send_message
    self._sendall_with_error_handle(header + data)
  File "/home/timssh/miniconda3/envs/tagging/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py", line 130, in _sendall_with_error_handle
    sent = self._sock.send(data)
BrokenPipeError: [Errno 32] Broken pipe
`Trainer.fit` stopped: `max_epochs=60` reached.
Traceback (most recent call last):
  File "/home/timssh/ML/TAGGING/CLS/classification/train_wb.py", line 105, in <module>
    path_ = WRAPPER.get_best_model(model)
  File "/home/timssh/ML/TAGGING/CLS/classification/train/service.py", line 136, in get_best_model
    extra_files = torch._C.ExtraFilesMap()
AttributeError: module 'torch._C' has no attribute 'ExtraFilesMap'
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Traceback (most recent call last) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ /home/timssh/ML/TAGGING/CLS/classification/train_wb.py:105 in <module>       â”‚
â”‚                                                                              â”‚
â”‚   102 â”‚   # Train the model âš¡                                               â”‚
â”‚   103 â”‚   trainer.fit(model, WRAPPER.train_loader, WRAPPER.val_loader)       â”‚
â”‚   104 â”‚                                                                      â”‚
â”‚ â± 105 â”‚   path_ = WRAPPER.get_best_model(model)                              â”‚
â”‚   106                                                                        â”‚
â”‚                                                                              â”‚
â”‚ /home/timssh/ML/TAGGING/CLS/classification/train/service.py:136 in           â”‚
â”‚ get_best_model                                                               â”‚
â”‚                                                                              â”‚
â”‚   133 â”‚   â”‚   â”‚   model.eval()                                               â”‚
â”‚   134 â”‚   â”‚   â”‚   script = torch.jit.script(model.model)                     â”‚
â”‚   135 â”‚   â”‚   â”‚   # save for use in production environment                   â”‚
â”‚ â± 136 â”‚   â”‚   â”‚   extra_files = torch._C.ExtraFilesMap()                     â”‚
â”‚   137 â”‚   â”‚   â”‚   extra_files["num2label.json"] = json.dumps(self.num2label) â”‚
â”‚   138 â”‚   â”‚   â”‚   torch.jit.save(script, path.replace("ckpt", "pt"), _extra_ â”‚
â”‚   139 â”‚   â”‚   return path_                                                   â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
AttributeError: module 'torch._C' has no attribute 'ExtraFilesMap'
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb: 
wandb: Run history:
wandb:                  LR â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–
wandb:               epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:     train_acc_micro â–…â–„â–„â–…â–…â–…â–…â–‚â–ƒâ–…â–„â–„â–…â–‚â–…â–‡â–‡â–‚â–†â–…â–ˆâ–…â–ƒâ–â–„â–„â–‡â–†â–…â–‚â–‡â–ƒâ–„â–…â–„â–‡â–…â–‡â–‡â–ˆ
wandb:          train_loss â–ˆâ–‚â–ƒâ–†â–†â–ˆâ–„â–ƒâ–…â–â–â–„â–ƒâ–„â–„â–â–ƒâ–…â–„â–ƒâ–â–„â–…â–…â–…â–…â–‚â–‚â–â–ƒâ–‚â–„â–â–…â–†â–‚â–‚â–â–„â–
wandb: trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: val_acc_micro_epoch â–â–…â–†â–‡â–ˆâ–‡â–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:  val_acc_micro_step â–â–ˆâ–ƒâ–ˆâ–ˆâ–ˆâ–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:      val_loss_epoch â–ˆâ–„â–ƒâ–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:       val_loss_step â–ˆâ–â–…â–â–â–â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                  LR 1e-05
wandb:               epoch 59
wandb:     train_acc_micro 0.75758
wandb:          train_loss 6.33249
wandb: trainer/global_step 6839
wandb: val_acc_micro_epoch 1.0
wandb:  val_acc_micro_step 1.0
wandb:      val_loss_epoch 0.00027
wandb:       val_loss_step 0.00022
wandb: 
wandb: ğŸš€ View run v__2_all_eff_36_0.001 at: https://wandb.ai/tsu/body_decoration_body_painting/runs/v__2_all_eff_36_0.001
wandb: ï¸âš¡ View job at https://wandb.ai/tsu/body_decoration_body_painting/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjk2MTUwNzEx/version_details/v1
wandb: Synced 6 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)
wandb: Find logs at: /home/timssh/ML/TAGGING/DATA/models/wandb/run-20230911_173143-v__2_all_eff_36_0.001/logs
Global seed set to 1
/home/timssh/miniconda3/envs/tagging/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:197: UserWarning: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.
  rank_zero_warn(
wandb: Currently logged in as: timasaviin (tsu). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.15.10
wandb: Run data is saved locally in /home/timssh/ML/TAGGING/DATA/models/wandb/run-20230912_065446-v__3_train_eff_36_0.001
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run v__3_train_eff_36_0.001
wandb: â­ï¸ View project at https://wandb.ai/tsu/body_decoration_body_painting
wandb: ğŸš€ View run at https://wandb.ai/tsu/body_decoration_body_painting/runs/v__3_train_eff_36_0.001
/home/timssh/miniconda3/envs/tagging/lib/python3.10/site-packages/lightning_fabric/connector.py:562: UserWarning: 16 is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!
  rank_zero_warn(
Using 16bit Automatic Mixed Precision (AMP)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]

  | Name          | Type              | Params
----------------------------------------------------
0 | model         | EfficientNet      | 20.2 M
1 | transform     | DataAugmentation  | 0     
2 | sigmoid       | Sigmoid           | 0     
3 | cross_entropy | BCEWithLogitsLoss | 0     
4 | accuracy1     | BinaryAccuracy    | 0     
----------------------------------------------------
20.2 M    Trainable params
0         Non-trainable params
20.2 M    Total params
80.715    Total estimated model params size (MB)
`Trainer.fit` stopped: `max_epochs=60` reached.
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                  LR â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:               epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:     train_acc_micro â–ˆâ–â–…â–â–†â–†â–„â–‚â–ˆâ–…â–…â–ƒâ–„â–‡â–…â–‡â–‡â–ˆâ–‡â–…â–†â–…â–…â–†â–‡â–‚â–…â–…â–‡â–‡â–„â–‡â–„â–‡â–„â–†â–…â–‡â–…â–‚
wandb:          train_loss â–‡â–ƒâ–ˆâ–‡â–â–…â–…â–„â–ƒâ–‡â–…â–…â–„â–â–„â–ƒâ–‚â–…â–…â–…â–„â–â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–„â–ˆâ–‡â–„â–â–„â–…â–‚â–†â–„â–…
wandb: trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–‚â–…â–…â–…â–…â–‚â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: val_acc_micro_epoch â–â–„â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–ˆâ–‡â–†â–‡â–‡â–‚â–‡â–‡â–ˆâ–†â–ˆâ–‡â–‡â–‡â–ˆâ–ˆâ–‡â–‡â–ˆâ–‡â–ˆâ–‡â–‡â–‡â–‡â–ˆâ–ˆâ–‡â–ˆâ–‡â–ˆ
wandb:  val_acc_micro_step â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–…â–…â–ˆâ–ˆâ–…â–ˆâ–…â–ˆâ–ˆâ–…â–…â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–…â–ˆâ–ˆâ–…â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–…â–
wandb:      val_loss_epoch â–ˆâ–…â–ƒâ–„â–‚â–‚â–„â–ƒâ–ƒâ–‚â–‚â–„â–ƒâ–„â–‡â–ƒâ–‚â–â–‚â–‚â–‚â–‚â–ƒâ–‚â–â–ƒâ–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–‚â–‚â–â–
wandb:       val_loss_step â–ˆâ–„â–â–ƒâ–â–â–‚â–ƒâ–â–â–â–â–„â–â–‚â–„â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–ƒâ–â–â–ƒâ–â–â–â–â–‚â–â–‚â–…
wandb: 
wandb: Run summary:
wandb:                  LR 5e-05
wandb:               epoch 59
wandb:     train_acc_micro 0.5
wandb:          train_loss 0.79769
wandb: trainer/global_step 5519
wandb: val_acc_micro_epoch 0.99517
wandb:  val_acc_micro_step 1.0
wandb:      val_loss_epoch 0.5578
wandb:       val_loss_step 0.07171
wandb: 
wandb: ğŸš€ View run v__3_train_eff_36_0.001 at: https://wandb.ai/tsu/body_decoration_body_painting/runs/v__3_train_eff_36_0.001
wandb: ï¸âš¡ View job at https://wandb.ai/tsu/body_decoration_body_painting/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjk2MTUwNzEx/version_details/v1
wandb: Synced 6 W&B file(s), 0 media file(s), 1 artifact file(s) and 0 other file(s)
wandb: Find logs at: /home/timssh/ML/TAGGING/DATA/models/wandb/run-20230912_065446-v__3_train_eff_36_0.001/logs
Global seed set to 1
/home/timssh/miniconda3/envs/tagging/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:197: UserWarning: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.
  rank_zero_warn(
wandb: Currently logged in as: timasaviin (tsu). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.15.10
wandb: Run data is saved locally in /home/timssh/ML/TAGGING/DATA/models/wandb/run-20230912_105510-v__1_train_eff_36_0.001
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run v__1_train_eff_36_0.001
wandb: â­ï¸ View project at https://wandb.ai/tsu/tits_size
wandb: ğŸš€ View run at https://wandb.ai/tsu/tits_size/runs/v__1_train_eff_36_0.001
/home/timssh/miniconda3/envs/tagging/lib/python3.10/site-packages/lightning_fabric/connector.py:562: UserWarning: 16 is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!
  rank_zero_warn(
Using 16bit Automatic Mixed Precision (AMP)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]

  | Name          | Type               | Params
-----------------------------------------------------
0 | model         | EfficientNet       | 20.2 M
1 | transform     | DataAugmentation   | 0     
2 | sigmoid       | Sigmoid            | 0     
3 | cross_entropy | BCEWithLogitsLoss  | 0     
4 | accuracy1     | MultilabelAccuracy | 0     
5 | F1_M          | MultilabelF1Score  | 0     
6 | F1_m          | MultilabelF1Score  | 0     
7 | F1_N          | MultilabelF1Score  | 0     
-----------------------------------------------------
20.2 M    Trainable params
0         Non-trainable params
20.2 M    Total params
80.730    Total estimated model params size (MB)
`Trainer.fit` stopped: `max_epochs=60` reached.
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                        LR â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–„â–„â–„â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–
wandb:                     epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:            train_F1_Macro â–â–â–â–‚â–ƒâ–ƒâ–„â–‚â–„â–ƒâ–‚â–„â–‚â–ƒâ–ƒâ–‚â–ƒâ–„â–ƒâ–…â–ƒâ–…â–ƒâ–…â–…â–…â–†â–…â–ƒâ–„â–†â–ƒâ–ˆâ–ˆâ–„â–ˆâ–„â–„â–†â–ˆ
wandb:            train_F1_micro â–â–â–â–‚â–‚â–‚â–„â–‚â–„â–‚â–ƒâ–„â–‚â–„â–ƒâ–ƒâ–ƒâ–…â–„â–…â–„â–…â–„â–…â–„â–„â–†â–…â–ƒâ–„â–…â–ƒâ–‡â–†â–…â–‡â–…â–„â–†â–ˆ
wandb:           train_acc_micro â–â–ƒâ–ƒâ–„â–„â–„â–…â–„â–…â–„â–…â–†â–ƒâ–…â–…â–…â–„â–†â–†â–†â–…â–†â–†â–‡â–†â–‡â–†â–‡â–…â–†â–†â–…â–‡â–‡â–†â–ˆâ–†â–†â–‡â–ˆ
wandb:         train_f1_big tits â–â–ƒâ–â–„â–„â–‚â–†â–‚â–„â–‚â–‚â–ƒâ–„â–†â–„â–‚â–…â–‚â–†â–ƒâ–†â–…â–‡â–…â–†â–‚â–…â–„â–ƒâ–…â–†â–…â–ˆâ–…â–…â–„â–†â–ƒâ–ˆâ–ˆ
wandb:     train_f1_flat chested â–ƒâ–â–‚â–‚â–„â–„â–„â–‚â–…â–„â–ƒâ–…â–ƒâ–…â–„â–ƒâ–ƒâ–…â–ƒâ–†â–„â–…â–ƒâ–…â–…â–ƒâ–‡â–ƒâ–…â–…â–„â–‚â–‡â–‡â–ƒâ–†â–„â–†â–…â–ˆ
wandb:        train_f1_huge tits â–â–ƒâ–â–â–†â–ƒâ–„â–†â–„â–„â–â–â–ƒâ–â–â–â–ƒâ–â–â–…â–â–…â–â–â–…â–ˆâ–â–‡â–â–â–„â–â–ˆâ–‡â–â–‡â–â–â–â–
wandb:       train_f1_small tits â–ƒâ–ƒâ–„â–„â–â–„â–ƒâ–„â–ƒâ–ƒâ–…â–†â–‚â–ƒâ–„â–†â–„â–‡â–…â–…â–„â–…â–„â–†â–„â–‡â–†â–‡â–…â–ƒâ–‡â–†â–„â–‡â–‡â–ˆâ–†â–†â–‡â–‡
wandb:                train_loss â–ˆâ–„â–†â–„â–ˆâ–„â–„â–ƒâ–…â–„â–„â–ƒâ–ˆâ–…â–„â–ƒâ–ˆâ–ƒâ–‚â–ƒâ–ƒâ–†â–‚â–ƒâ–„â–â–ƒâ–â–„â–‚â–…â–ƒâ–‚â–‚â–ƒâ–ƒâ–‚â–ƒâ–â–
wandb:       trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–â–ƒâ–ƒâ–„â–„â–‚â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–‚â–†â–†â–†â–‡â–‚â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:        val_F1_Macro_epoch â–â–‚â–‚â–ƒâ–„â–„â–…â–„â–…â–…â–†â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–†â–‡â–‡â–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:         val_F1_Macro_step â–â–â–‚â–‚â–ƒâ–‚â–ƒâ–ƒâ–ƒâ–‚â–†â–ƒâ–„â–ƒâ–…â–ƒâ–…â–ƒâ–†â–„â–„â–†â–ƒâ–‡â–…â–ˆâ–ƒâ–‡â–ƒâ–‡â–„â–‡â–„â–ˆâ–„â–ˆâ–…â–ˆâ–…â–„
wandb:        val_F1_micro_epoch â–â–‚â–ƒâ–ƒâ–„â–„â–…â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:         val_F1_micro_step â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–„â–„â–…â–†â–„â–…â–†â–†â–…â–…â–†â–†â–‡â–†â–†â–‡â–ˆâ–‡â–†â–‡â–…â–†â–‡â–‡â–‡â–‡â–ˆâ–‡â–ˆâ–‡â–ˆâ–‡
wandb:       val_acc_micro_epoch â–â–ƒâ–ƒâ–„â–…â–…â–†â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:        val_acc_micro_step â–â–ƒâ–ƒâ–„â–„â–…â–…â–†â–…â–…â–†â–‡â–†â–†â–†â–‡â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:     val_f1_big tits_epoch â–â–â–‚â–„â–„â–ƒâ–„â–„â–…â–…â–†â–…â–†â–…â–†â–†â–†â–…â–‡â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡
wandb:      val_f1_big tits_step â–â–â–‚â–ƒâ–ƒâ–ƒâ–„â–‡â–…â–„â–†â–‡â–…â–…â–…â–…â–„â–„â–…â–ƒâ–„â–„â–…â–†â–‡â–…â–„â–†â–„â–…â–ˆâ–†â–‡â–†â–ˆâ–†â–ˆâ–†â–ˆâ–‡
wandb: val_f1_flat chested_epoch â–â–‚â–‚â–ƒâ–ƒâ–…â–„â–…â–„â–…â–†â–†â–…â–…â–†â–‡â–†â–†â–‡â–‡â–‡â–‡â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:  val_f1_flat chested_step â–â–ƒâ–‚â–„â–‚â–„â–ƒâ–„â–ƒâ–ƒâ–„â–…â–„â–„â–†â–…â–…â–…â–…â–†â–†â–‡â–„â–ˆâ–†â–ˆâ–…â–‡â–†â–‡â–†â–†â–†â–ˆâ–†â–ˆâ–†â–ˆâ–…â–†
wandb:    val_f1_huge tits_epoch â–â–ƒâ–‚â–ƒâ–…â–†â–…â–ƒâ–†â–…â–†â–…â–…â–†â–…â–‡â–‡â–…â–‡â–‡â–‡â–†â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:     val_f1_huge tits_step â–‚â–â–‚â–â–…â–â–ƒâ–â–„â–â–ˆâ–â–„â–â–…â–â–†â–â–†â–â–â–†â–â–†â–â–ˆâ–â–ˆâ–â–ˆâ–â–ˆâ–â–ˆâ–â–ˆâ–â–ˆâ–â–
wandb:   val_f1_small tits_epoch â–â–â–ƒâ–‚â–ƒâ–„â–…â–…â–…â–„â–„â–…â–†â–†â–†â–†â–…â–…â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    val_f1_small tits_step â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–„â–‚â–ƒâ–…â–„â–ƒâ–…â–„â–…â–ƒâ–†â–…â–†â–‡â–…â–…â–…â–ˆâ–„â–†â–„â–ƒâ–„â–…â–…â–„â–…â–†â–†â–‡â–†â–ˆâ–…
wandb:            val_loss_epoch â–„â–ƒâ–ƒâ–‚â–‚â–‚â–â–‚â–â–â–‚â–â–‚â–â–‚â–‚â–‚â–‚â–ƒâ–‚â–ƒâ–„â–ƒâ–…â–„â–…â–„â–…â–†â–†â–†â–‡â–…â–†â–ˆâ–‡â–ˆâ–‡â–ˆâ–‡
wandb:             val_loss_step â–†â–‚â–‡â–‚â–„â–‚â–„â–‚â–„â–‚â–ƒâ–â–„â–‚â–ƒâ–â–ƒâ–‚â–…â–‚â–‚â–„â–â–„â–â–†â–â–ˆâ–‚â–†â–‚â–ˆâ–‚â–‡â–‚â–ˆâ–‚â–ˆâ–‚â–‚
wandb: 
wandb: Run summary:
wandb:                        LR 1e-05
wandb:                     epoch 59
wandb:            train_F1_Macro 0.58929
wandb:            train_F1_micro 0.71429
wandb:           train_acc_micro 0.83333
wandb:         train_f1_big tits 0.5
wandb:     train_f1_flat chested 1.0
wandb:        train_f1_huge tits 0.0
wandb:       train_f1_small tits 0.85714
wandb:                train_loss 21.1631
wandb:       trainer/global_step 30659
wandb:        val_F1_Macro_epoch 0.68228
wandb:         val_F1_Macro_step 0.5997
wandb:        val_F1_micro_epoch 0.80732
wandb:         val_F1_micro_step 0.8125
wandb:       val_acc_micro_epoch 0.92183
wandb:        val_acc_micro_step 0.925
wandb:     val_f1_big tits_epoch 0.85322
wandb:      val_f1_big tits_step 0.875
wandb: val_f1_flat chested_epoch 0.76556
wandb:  val_f1_flat chested_step 0.85714
wandb:    val_f1_huge tits_epoch 0.3888
wandb:     val_f1_huge tits_step 0.0
wandb:   val_f1_small tits_epoch 0.72153
wandb:    val_f1_small tits_step 0.66667
wandb:            val_loss_epoch 249.53261
wandb:             val_loss_step 55.72646
wandb: 
wandb: ğŸš€ View run v__1_train_eff_36_0.001 at: https://wandb.ai/tsu/tits_size/runs/v__1_train_eff_36_0.001
wandb: ï¸âš¡ View job at https://wandb.ai/tsu/tits_size/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjkwMjAxNDA5/version_details/v1
wandb: Synced 6 W&B file(s), 0 media file(s), 1 artifact file(s) and 0 other file(s)
wandb: Find logs at: /home/timssh/ML/TAGGING/DATA/models/wandb/run-20230912_105510-v__1_train_eff_36_0.001/logs
Global seed set to 1
/home/timssh/miniconda3/envs/tagging/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:197: UserWarning: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.
  rank_zero_warn(
wandb: Currently logged in as: timasaviin (tsu). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.15.10
wandb: Run data is saved locally in /home/timssh/ML/TAGGING/DATA/models/wandb/run-20230913_060922-v__2_all_eff_36_0.001
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run v__2_all_eff_36_0.001
wandb: â­ï¸ View project at https://wandb.ai/tsu/tits_size
wandb: ğŸš€ View run at https://wandb.ai/tsu/tits_size/runs/v__2_all_eff_36_0.001
/home/timssh/miniconda3/envs/tagging/lib/python3.10/site-packages/lightning_fabric/connector.py:562: UserWarning: 16 is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!
  rank_zero_warn(
Using 16bit Automatic Mixed Precision (AMP)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]

  | Name          | Type               | Params
-----------------------------------------------------
0 | model         | EfficientNet       | 20.2 M
1 | transform     | DataAugmentation   | 0     
2 | sigmoid       | Sigmoid            | 0     
3 | cross_entropy | BCEWithLogitsLoss  | 0     
4 | accuracy1     | MultilabelAccuracy | 0     
5 | F1_M          | MultilabelF1Score  | 0     
6 | F1_m          | MultilabelF1Score  | 0     
7 | F1_N          | MultilabelF1Score  | 0     
-----------------------------------------------------
20.2 M    Trainable params
0         Non-trainable params
20.2 M    Total params
80.730    Total estimated model params size (MB)
`Trainer.fit` stopped: `max_epochs=60` reached.
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                        LR â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–â–â–â–
wandb:                     epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:            train_F1_Macro â–‚â–‚â–ƒâ–„â–ƒâ–‚â–ƒâ–†â–â–„â–ƒâ–â–…â–ƒâ–ƒâ–…â–„â–‚â–ƒâ–…â–…â–„â–…â–„â–…â–„â–…â–„â–†â–…â–„â–†â–„â–†â–…â–„â–ˆâ–„â–„â–„
wandb:            train_F1_micro â–‚â–‚â–ƒâ–ƒâ–ƒâ–‚â–ƒâ–…â–â–…â–ƒâ–ƒâ–…â–ƒâ–„â–†â–…â–ƒâ–„â–…â–†â–…â–…â–„â–†â–„â–…â–„â–‡â–…â–…â–‡â–…â–ˆâ–†â–…â–ˆâ–„â–…â–…
wandb:           train_acc_micro â–â–â–ƒâ–„â–ƒâ–ƒâ–„â–…â–‚â–…â–„â–„â–†â–„â–…â–‡â–†â–„â–…â–…â–‡â–…â–…â–…â–†â–…â–…â–…â–‡â–†â–†â–†â–…â–ˆâ–‡â–†â–ˆâ–„â–…â–†
wandb:         train_f1_big tits â–„â–‚â–ƒâ–ƒâ–…â–‚â–â–…â–â–†â–‚â–‡â–†â–„â–†â–‡â–ƒâ–‡â–†â–…â–…â–…â–†â–„â–ƒâ–†â–†â–…â–‡â–„â–‡â–†â–…â–ˆâ–†â–…â–†â–…â–‡â–…
wandb:     train_f1_flat chested â–â–†â–†â–…â–†â–„â–†â–†â–…â–…â–…â–‚â–ˆâ–†â–„â–„â–…â–…â–ƒâ–‡â–‡â–…â–ƒâ–„â–ˆâ–†â–†â–ƒâ–ˆâ–…â–…â–ˆâ–„â–‡â–‡â–„â–†â–†â–„â–†
wandb:        train_f1_huge tits â–‚â–â–â–ƒâ–â–â–â–†â–â–â–„â–â–…â–â–â–…â–â–â–â–ƒâ–â–â–ˆâ–â–â–â–…â–ƒâ–â–ƒâ–â–â–ƒâ–â–â–â–ˆâ–â–â–
wandb:       train_f1_small tits â–…â–ƒâ–„â–„â–„â–„â–…â–…â–ƒâ–…â–„â–â–‚â–„â–…â–„â–ˆâ–â–…â–…â–‡â–†â–…â–‡â–‡â–„â–ƒâ–†â–†â–‡â–†â–†â–‡â–‡â–…â–‡â–ˆâ–…â–†â–„
wandb:                train_loss â–ƒâ–ƒâ–ƒâ–ˆâ–ƒâ–ƒâ–ƒâ–„â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–…â–ƒâ–‚â–‚â–„â–‚â–‚â–‚â–‚â–‚â–„â–â–‚â–‚â–â–‚â–‚â–
wandb:       trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:        val_F1_Macro_epoch â–â–â–‚â–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–„â–…â–„â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:         val_F1_Macro_step â–‚â–ƒâ–‚â–„â–‚â–‚â–ƒâ–â–…â–ƒâ–ƒâ–„â–„â–†â–†â–…â–„â–†â–„â–„â–„â–†â–†â–†â–„â–‡â–‡â–…â–„â–ˆâ–‡â–…â–‚â–‡â–…â–‡â–ˆâ–ˆâ–„â–„
wandb:        val_F1_micro_epoch â–â–â–‚â–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–„â–„â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:         val_F1_micro_step â–â–â–‚â–ƒâ–‚â–ƒâ–„â–â–ƒâ–„â–…â–…â–„â–„â–„â–…â–…â–†â–†â–†â–‡â–‡â–†â–†â–‡â–‡â–‡â–‡â–†â–ˆâ–‡â–ˆâ–‡â–†â–ˆâ–‡â–‡â–ˆâ–‡â–‡
wandb:       val_acc_micro_epoch â–â–‚â–ƒâ–„â–„â–…â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:        val_acc_micro_step â–â–‚â–‚â–„â–ƒâ–ƒâ–…â–ƒâ–…â–…â–…â–†â–†â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:     val_f1_big tits_epoch â–â–â–‚â–ƒâ–ƒâ–„â–„â–ƒâ–„â–…â–…â–„â–ƒâ–…â–…â–…â–†â–†â–†â–†â–‡â–†â–‡â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:      val_f1_big tits_step â–‚â–â–â–„â–„â–ƒâ–„â–‚â–‚â–„â–‡â–„â–„â–‚â–‚â–ƒâ–‡â–†â–†â–…â–‡â–†â–ƒâ–…â–†â–‡â–‡â–‡â–†â–ˆâ–‡â–‡â–‡â–‡â–ˆâ–‡â–†â–ˆâ–‡â–‡
wandb: val_f1_flat chested_epoch â–â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–ƒâ–ƒâ–„â–„â–…â–„â–…â–†â–†â–†â–†â–‡â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:  val_f1_flat chested_step â–…â–…â–…â–†â–…â–…â–†â–‚â–†â–†â–†â–‡â–…â–‡â–…â–‡â–‡â–†â–†â–ˆâ–‡â–…â–‡â–‡â–†â–‡â–†â–‡â–ˆâ–‡â–‡â–ˆâ–â–†â–‡â–‡â–ˆâ–ˆâ–‡â–‡
wandb:    val_f1_huge tits_epoch â–â–â–‚â–„â–‚â–…â–…â–†â–„â–…â–†â–†â–†â–…â–†â–‡â–†â–‡â–‡â–‡â–ˆâ–ˆâ–‡â–‡â–‡â–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:     val_f1_huge tits_step â–ƒâ–„â–â–†â–â–â–â–â–ˆâ–â–â–â–„â–ˆâ–ˆâ–…â–â–†â–â–â–â–†â–†â–‡â–â–ˆâ–ˆâ–â–â–ˆâ–ˆâ–â–â–ˆâ–â–ˆâ–ˆâ–ˆâ–â–
wandb:   val_f1_small tits_epoch â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–…â–†â–†â–†â–†â–†â–‡â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:    val_f1_small tits_step â–â–‚â–„â–ƒâ–„â–…â–ƒâ–„â–„â–„â–ƒâ–…â–†â–„â–†â–„â–„â–‡â–‡â–…â–‡â–ˆâ–‡â–‡â–‡â–‡â–†â–†â–†â–ˆâ–‡â–ˆâ–‡â–†â–‡â–†â–‡â–‡â–‡â–‡
wandb:            val_loss_epoch â–ˆâ–‡â–†â–…â–…â–„â–…â–„â–„â–„â–ƒâ–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:             val_loss_step â–ˆâ–ˆâ–‡â–…â–†â–ˆâ–„â–ˆâ–…â–„â–„â–ƒâ–„â–ˆâ–ƒâ–„â–…â–‚â–‚â–ƒâ–â–‚â–‚â–‚â–‚â–â–â–â–‚â–â–â–â–â–‚â–â–‚â–â–â–‚â–
wandb: 
wandb: Run summary:
wandb:                        LR 3e-05
wandb:                     epoch 59
wandb:            train_F1_Macro 0.45281
wandb:            train_F1_micro 0.52632
wandb:           train_acc_micro 0.65385
wandb:         train_f1_big tits 0.76923
wandb:     train_f1_flat chested 0.26667
wandb:        train_f1_huge tits 0.16667
wandb:       train_f1_small tits 0.6087
wandb:                train_loss 175.81104
wandb:       trainer/global_step 38279
wandb:        val_F1_Macro_epoch 0.83542
wandb:         val_F1_Macro_step 0.75
wandb:        val_F1_micro_epoch 0.96366
wandb:         val_F1_micro_step 1.0
wandb:       val_acc_micro_epoch 0.98577
wandb:        val_acc_micro_step 1.0
wandb:     val_f1_big tits_epoch 0.98392
wandb:      val_f1_big tits_step 1.0
wandb: val_f1_flat chested_epoch 0.93336
wandb:  val_f1_flat chested_step 1.0
wandb:    val_f1_huge tits_epoch 0.48933
wandb:     val_f1_huge tits_step 0.0
wandb:   val_f1_small tits_epoch 0.93507
wandb:    val_f1_small tits_step 1.0
wandb:            val_loss_epoch 8.46111
wandb:             val_loss_step 0.14482
wandb: 
wandb: ğŸš€ View run v__2_all_eff_36_0.001 at: https://wandb.ai/tsu/tits_size/runs/v__2_all_eff_36_0.001
wandb: ï¸âš¡ View job at https://wandb.ai/tsu/tits_size/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjkwMjAxNDA5/version_details/v1
wandb: Synced 6 W&B file(s), 0 media file(s), 1 artifact file(s) and 0 other file(s)
wandb: Find logs at: /home/timssh/ML/TAGGING/DATA/models/wandb/run-20230913_060922-v__2_all_eff_36_0.001/logs
Global seed set to 1
/home/timssh/miniconda3/envs/tagging/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:197: UserWarning: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.
  rank_zero_warn(
wandb: Currently logged in as: timasaviin (tsu). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.15.10
wandb: Run data is saved locally in /home/timssh/ML/TAGGING/DATA/models/wandb/run-20230915_092749-v__0_train_eff_36_0.001
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run v__0_train_eff_36_0.001
wandb: â­ï¸ View project at https://wandb.ai/tsu/sex_positions
wandb: ğŸš€ View run at https://wandb.ai/tsu/sex_positions/runs/v__0_train_eff_36_0.001
/home/timssh/miniconda3/envs/tagging/lib/python3.10/site-packages/lightning_fabric/connector.py:562: UserWarning: 16 is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!
  rank_zero_warn(
Using 16bit Automatic Mixed Precision (AMP)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]

  | Name          | Type               | Params
-----------------------------------------------------
0 | model         | EfficientNet       | 20.2 M
1 | transform     | DataAugmentation   | 0     
2 | sigmoid       | Sigmoid            | 0     
3 | cross_entropy | BCEWithLogitsLoss  | 0     
4 | accuracy1     | MultilabelAccuracy | 0     
5 | F1_M          | MultilabelF1Score  | 0     
6 | F1_m          | MultilabelF1Score  | 0     
7 | F1_N          | MultilabelF1Score  | 0     
-----------------------------------------------------
20.2 M    Trainable params
0         Non-trainable params
20.2 M    Total params
80.741    Total estimated model params size (MB)
`Trainer.fit` stopped: `max_epochs=60` reached.
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                           LR â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–„â–„â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–
wandb:                        epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:               train_F1_Macro â–ƒâ–â–…â–†â–‡â–ƒâ–â–†â–„â–ˆâ–…â–‚â–â–ƒâ–‡â–…â–‚â–„â–„â–‚â–‚â–„â–ƒâ–†â–„â–ƒâ–ƒâ–‚â–†â–†â–‡â–â–…â–„â–„â–„â–…â–ƒâ–…â–…
wandb:               train_F1_micro â–ƒâ–‚â–†â–†â–‡â–ƒâ–‚â–‡â–„â–ˆâ–‡â–‚â–â–ƒâ–†â–…â–‚â–…â–„â–‚â–‚â–„â–„â–‡â–„â–„â–„â–‚â–†â–†â–†â–â–†â–„â–…â–…â–„â–ƒâ–…â–…
wandb:              train_acc_micro â–„â–â–†â–†â–‡â–ƒâ–ƒâ–†â–„â–‡â–ˆâ–‚â–â–…â–‡â–‡â–â–„â–…â–ƒâ–„â–…â–…â–‡â–…â–†â–„â–â–…â–†â–†â–„â–†â–ƒâ–…â–…â–…â–„â–†â–†
wandb:                  train_f1_69 â–„â–‚â–‡â–ƒâ–ˆâ–…â–ƒâ–ƒâ–ƒâ–‡â–â–‚â–„â–„â–…â–„â–â–‚â–…â–„â–‡â–…â–â–…â–…â–ƒâ–…â–ƒâ–‡â–‡â–…â–„â–â–†â–ƒâ–„â–…â–ƒâ–…â–„
wandb:             train_f1_cowgirl â–â–â–ƒâ–„â–†â–â–„â–‡â–…â–…â–ˆâ–â–„â–†â–†â–„â–…â–„â–†â–„â–„â–…â–‚â–†â–„â–ˆâ–‡â–‚â–„â–„â–†â–ƒâ–‡â–„â–ˆâ–ƒâ–‡â–„â–†â–„
wandb:         train_f1_doggy style â–ˆâ–†â–ˆâ–‡â–ƒâ–…â–…â–‡â–„â–‡â–ƒâ–‡â–ƒâ–„â–†â–…â–…â–‡â–â–ƒâ–‚â–„â–„â–‡â–‡â–ƒâ–ƒâ–†â–…â–…â–…â–…â–†â–„â–†â–ƒâ–…â–…â–‡â–†
wandb:          train_f1_missionary â–„â–ƒâ–…â–†â–„â–„â–‚â–…â–‚â–…â–ˆâ–‚â–ƒâ–„â–‡â–…â–ƒâ–‚â–†â–ƒâ–ƒâ–†â–†â–‡â–„â–ƒâ–ƒâ–â–…â–ƒâ–…â–…â–‡â–„â–ƒâ–†â–…â–‚â–ƒâ–‡
wandb:     train_f1_reverse cowgirl â–‚â–ƒâ–â–„â–‡â–…â–â–†â–‡â–‡â–…â–…â–‚â–â–…â–ˆâ–„â–†â–„â–„â–‚â–ƒâ–†â–â–ƒâ–„â–…â–„â–…â–…â–†â–â–„â–ƒâ–ƒâ–†â–…â–‡â–„â–…
wandb:            train_f1_spooning â–…â–…â–†â–ˆâ–…â–‚â–…â–‡â–†â–ˆâ–†â–ƒâ–ƒâ–…â–„â–‚â–ƒâ–†â–„â–‚â–ƒâ–†â–„â–†â–„â–…â–…â–‡â–†â–‡â–†â–â–†â–…â–…â–…â–â–‚â–â–„
wandb:                   train_loss â–ƒâ–ˆâ–ƒâ–ƒâ–†â–…â–…â–„â–…â–…â–â–…â–…â–‚â–ƒâ–„â–…â–†â–ƒâ–†â–†â–‡â–‚â–„â–†â–ƒâ–…â–†â–ƒâ–ƒâ–ƒâ–‚â–ƒâ–ˆâ–„â–„â–ƒâ–…â–„â–„
wandb:          trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–â–ƒâ–â–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–‚â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‚â–‡â–ƒâ–ˆâ–ˆ
wandb:           val_F1_Macro_epoch â–â–‚â–„â–…â–†â–…â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:            val_F1_Macro_step â–‚â–ƒâ–†â–†â–„â–†â–‡â–…â–‡â–‡â–„â–…â–‡â–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–†â–‡â–‡â–‡â–ˆâ–‡â–ˆâ–‡â–†â–†â–‡â–†â–‡â–…â–‡â–â–‡â–‡â–‡â–‡â–ˆ
wandb:           val_F1_micro_epoch â–â–‚â–„â–…â–†â–…â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:            val_F1_micro_step â–â–ƒâ–†â–†â–†â–…â–‡â–‡â–‡â–‡â–…â–‡â–‡â–ˆâ–†â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–†â–ˆâ–‡â–ˆâ–‡â–ˆâ–‡â–‡â–ˆâ–‡â–‡â–‡â–†â–‡â–‡â–ˆâ–‡â–ˆ
wandb:          val_acc_micro_epoch â–â–ƒâ–…â–†â–‡â–†â–‡â–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:           val_acc_micro_step â–â–ƒâ–†â–†â–†â–…â–‡â–‡â–‡â–‡â–†â–‡â–‡â–ˆâ–†â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–ˆâ–‡â–ˆâ–‡â–ˆâ–‡â–‡â–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–ˆ
wandb:              val_f1_69_epoch â–â–â–„â–„â–‡â–‡â–‡â–ˆâ–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:               val_f1_69_step â–…â–†â–ˆâ–†â–†â–ˆâ–ˆâ–‡â–ˆâ–ˆâ–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–â–†â–†â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–†â–ˆâ–ˆâ–‡â–ˆâ–â–ˆâ–ˆâ–ˆâ–‡â–ˆ
wandb:         val_f1_cowgirl_epoch â–â–‚â–„â–…â–†â–…â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:          val_f1_cowgirl_step â–â–…â–ˆâ–ˆâ–†â–‡â–‡â–†â–ˆâ–‡â–‡â–‡â–ˆâ–ˆâ–‡â–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–â–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:     val_f1_doggy style_epoch â–â–„â–†â–‡â–‡â–†â–…â–‡â–‡â–‡â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:      val_f1_doggy style_step â–„â–‡â–‡â–ˆâ–‡â–†â–‡â–ˆâ–‡â–ˆâ–†â–‡â–†â–ˆâ–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–†â–ˆâ–â–ˆâ–ˆâ–†â–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:      val_f1_missionary_epoch â–â–ƒâ–‚â–…â–„â–â–…â–†â–†â–‡â–ˆâ–‡â–‡â–‡â–ˆâ–‡â–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:       val_f1_missionary_step â–†â–…â–…â–ˆâ–â–‡â–‡â–â–ˆâ–‡â–ˆâ–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–â–ˆâ–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: val_f1_reverse cowgirl_epoch â–‚â–â–ƒâ–…â–†â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–ˆâ–ˆâ–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–‡â–ˆâ–ˆ
wandb:  val_f1_reverse cowgirl_step â–‡â–…â–ˆâ–†â–ˆâ–‡â–ˆâ–ˆâ–…â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–‡â–†â–â–ˆâ–‡â–†â–ˆâ–ˆ
wandb:        val_f1_spooning_epoch â–â–„â–‚â–„â–†â–†â–‡â–‚â–‡â–ˆâ–‡â–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:         val_f1_spooning_step â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–â–ˆâ–‡â–ˆâ–ˆâ–‡â–ˆâ–ˆâ–‡â–ˆâ–ˆâ–‡â–ˆ
wandb:               val_loss_epoch â–ˆâ–…â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–‚â–â–‚â–â–â–‚â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–‚â–‚â–ƒâ–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–ƒâ–„â–„
wandb:                val_loss_step â–ƒâ–ƒâ–‚â–ˆâ–‚â–‡â–â–‚â–â–‚â–…â–‚â–ƒâ–â–†â–‚â–†â–â–â–â–â–â–ƒâ–â–‚â–â–â–â–‚â–â–â–‚â–†â–‚â–ˆâ–â–…â–‚â–â–
wandb: 
wandb: Run summary:
wandb:                           LR 1e-05
wandb:                        epoch 59
wandb:               train_F1_Macro 0.39423
wandb:               train_F1_micro 0.46154
wandb:              train_acc_micro 0.70833
wandb:                  train_f1_69 0.0
wandb:             train_f1_cowgirl 0.72727
wandb:         train_f1_doggy style 0.33333
wandb:          train_f1_missionary 0.33333
wandb:     train_f1_reverse cowgirl 0.4
wandb:            train_f1_spooning 0.57143
wandb:                   train_loss 64.15136
wandb:          trainer/global_step 37079
wandb:           val_F1_Macro_epoch 0.9038
wandb:            val_F1_Macro_step 0.66667
wandb:           val_F1_micro_epoch 0.95636
wandb:            val_F1_micro_step 1.0
wandb:          val_acc_micro_epoch 0.99253
wandb:           val_acc_micro_step 1.0
wandb:              val_f1_69_epoch 0.86097
wandb:               val_f1_69_step 0.0
wandb:         val_f1_cowgirl_epoch 0.88122
wandb:          val_f1_cowgirl_step 1.0
wandb:     val_f1_doggy style_epoch 0.94332
wandb:      val_f1_doggy style_step 1.0
wandb:      val_f1_missionary_epoch 0.87183
wandb:       val_f1_missionary_step 0.0
wandb: val_f1_reverse cowgirl_epoch 0.90905
wandb:  val_f1_reverse cowgirl_step 1.0
wandb:        val_f1_spooning_epoch 0.95642
wandb:         val_f1_spooning_step 1.0
wandb:               val_loss_epoch 50.33625
wandb:                val_loss_step 0.00517
wandb: 
wandb: ğŸš€ View run v__0_train_eff_36_0.001 at: https://wandb.ai/tsu/sex_positions/runs/v__0_train_eff_36_0.001
wandb: ï¸âš¡ View job at https://wandb.ai/tsu/sex_positions/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjk3ODIzNTkw/version_details/v0
wandb: Synced 6 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)
wandb: Find logs at: /home/timssh/ML/TAGGING/DATA/models/wandb/run-20230915_092749-v__0_train_eff_36_0.001/logs
